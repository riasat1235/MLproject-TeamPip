{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"source":["# Introduction\n","\n","This notebook aims at discovering Convolutional Neural Network. We will see the theory behind it, and an implementation in Pytorch for hand-digits classification on MNIST dataset. "]},{"cell_type":"markdown","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"source":["# History\n","\n","Contrary to what most people think, Neural Networks is quite an old concept. It was first introduced in 1957 under the name ***perceptron***. Peceptron is a 1-layer feed forward neural network. However the infrastructure and the algorthm around it was not good enough to allow large scale training. Later on in 1986, ***Multi Layer Perceptron (MLP)*** was introduced with the backpropagation algorithm in order to train a network with more than 1 layer. Thanks to this algorithm we are not able to train non-linear model which can learn high level abstract features. Then ***Convolutional Neural Network (CNN)*** has been introduced in order to learn better features and with the possibility to reduce the number of parameters to be trained. And now, here we are, in the ***Deep Learning era*** "]},{"cell_type":"markdown","metadata":{"_uuid":"b3124a67ef42274e81ddbd3d46d4937a3e2dfaa5"},"source":["    # Multi-Layer Perceptron\n","\n","The first thing to ask is : why do we needed Convolutional Neural Network in the first place... Well, let's see what happen when we train a Multi-Layer Perceptron to recognize hand-written digits. In Machine Learning we have our own \"Hello World\" which is the MNIST dataset. Let's see what this dataset is about and how a multi-layer perceptron will perform.   "]},{"cell_type":"code","execution_count":59,"metadata":{"_uuid":"46ade2a1807aadd90e577c496d77d3df507dad88","trusted":true},"outputs":[],"source":["import numpy as np # to handle matrix and data operation\n","import pandas as pd # to read csv and handle dataframe\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.data\n","from torch.autograd import Variable\n","\n","from sklearn.model_selection import train_test_split\n","import cv2\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import normalize"]},{"cell_type":"code","execution_count":60,"metadata":{"_uuid":"a87c9de979fb54874e3a047d40cc024a8b0f5e98","trusted":true},"outputs":[],"source":["data = np.load('Data/data_train.npy').T\n","X_og = data.copy()/255\n","y =np.loadtxt('Data/correct_labels.npy')"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.63137255 0.63529412 0.63529412 ... 0.58431373 0.58431373 0.57647059]\n"," [0.63529412 0.63529412 0.63529412 ... 0.64705882 0.64705882 0.64705882]\n"," [0.76470588 0.76470588 0.76470588 ... 0.74901961 0.74901961 0.74901961]\n"," ...\n"," [0.76470588 0.76470588 0.76470588 ... 0.76470588 0.76470588 0.76470588]\n"," [0.61960784 0.61568627 0.61568627 ... 0.55294118 0.54901961 0.55294118]\n"," [0.58431373 0.58431373 0.58823529 ... 0.54117647 0.55686275 0.57647059]]\n"]}],"source":["print(X_og)"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[8 8 1 ... 1 3 3]\n"]}],"source":["y = y.astype(float).astype(int)\n","print(y)"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(6720, 90000)\n","(6720,)\n"]}],"source":["print(np.shape(X_og))\n","print(np.shape(y))"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["#X = 1-X_og.copy()\n","def resize_func(input_data,new_width,new_height):#input the (1,90000) data\n","    size1 = np.shape(input_data)[0]\n","    size2 = int(size1**(0.5))\n","    output = cv2.resize(input_data.reshape(size2,size2),(new_width,new_height))\n","    return output\n","def morph_ops(input_data):\n","    dilate_kernel = np.ones((2,2),np.uint8)\n","    x0 = 1-input_data\n","    x0_1 = resize_func(x0,50,50)\n","    x1 = x0_1.reshape(50,50)\n","    x4 = np.clip(x1-np.mean(x1),0,1)\n","    x5 = np.where(x1 < 0.35,0,x4)\n","    dilate_kernel = np.ones((2,2),np.uint8)\n","    x6 = cv2.dilate(x5,dilate_kernel,iterations=1)\n","    x7 = np.where(x6<0.15,0,x6)\n","    x8 = normalize(x7.reshape(-1,1)).reshape(50,50)\n","    x9 = np.where(x8>0.3,1,x8)\n","    x10 = x9.reshape(1,-1)\n","    picture = x10\n","    return picture\n","\n","\n","X_new = np.zeros((np.shape(X_og)[0],50*50))\n","\n","for ii in range(np.shape(X_new)[0]):\n","    newrow = morph_ops(X_og[ii,:])\n","    newrow = newrow.reshape(1,-1)\n","    newrow = resize_func(newrow[0,:],50,50)\n","    newrow = newrow.reshape(1,-1)\n","    X_new[ii,:] = newrow\n","X = X_new"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"data":{"text/plain":["<matplotlib.image.AxesImage at 0x2544f82e520>"]},"execution_count":65,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["index = 0\n","x1 = X[index,:].reshape(50,50)\n","plt.imshow(x1,cmap='gray')"]},{"cell_type":"code","execution_count":66,"metadata":{"_uuid":"0b3d551a62defaadd37e681511ebc5fc70ac944d","trusted":true},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"]},{"cell_type":"code","execution_count":67,"metadata":{"_uuid":"96f0d5dbc90457eb091fb2e6ed68ce7c7bf6da0b","trusted":true},"outputs":[],"source":["BATCH_SIZE = 200\n","\n","torch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\n","torch_y_train = torch.from_numpy(y_train).type(torch.LongTensor) # data type is long\n","\n","# create feature and targets tensor for test set.\n","torch_X_test = torch.from_numpy(X_test).type(torch.LongTensor)\n","torch_y_test = torch.from_numpy(y_test).type(torch.LongTensor) # data type is long\n","\n","# Pytorch train and test sets\n","train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n","test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n","\n","# data loader\n","train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n","test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)\n"]},{"cell_type":"code","execution_count":68,"metadata":{"_uuid":"3624b779d746e6b5710711c7b1798363ecabafbb","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP(\n","  (linear1): Linear(in_features=2500, out_features=500, bias=True)\n","  (linear2): Linear(in_features=500, out_features=100, bias=True)\n","  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",")\n"]}],"source":["class MLP(nn.Module):\n","    def __init__(self):\n","        super(MLP, self).__init__()\n","        self.linear1 = nn.Linear(50*50,500)\n","        self.linear2 = nn.Linear(500,100)\n","        self.linear3 = nn.Linear(100,10)\n","    \n","    def forward(self,X):\n","        X = F.relu(self.linear1(X))\n","        X = F.relu(self.linear2(X))\n","        X = self.linear3(X)\n","        return F.log_softmax(X, dim=1)\n"," \n","mlp = MLP()\n","print(mlp)\n"]},{"cell_type":"markdown","metadata":{"_uuid":"6c2c74a49c925222b6aaec5e29007a302d2fae44"},"source":["We have 784\\*(250+1) + 250\\*(100+1) + 100\\*(10+1) = 222 360 parameters to train"]},{"cell_type":"code","execution_count":69,"metadata":{"_uuid":"e471e447a9618edc7310bb15941054c30ea235a4","trusted":true},"outputs":[],"source":["def fit(model, train_loader):\n","    optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)#,lr=0.001, betas=(0.9,0.999))\n","    error = nn.CrossEntropyLoss()\n","    EPOCHS = 250\n","    model.train()\n","    lossvec = []\n","    for epoch in range(EPOCHS):\n","        correct = 0\n","        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n","            var_X_batch = Variable(X_batch).float()\n","            var_y_batch = Variable(y_batch)\n","            optimizer.zero_grad()\n","            output = model(var_X_batch)\n","            loss = error(output, var_y_batch)\n","            loss.backward()\n","            lossvec.append(loss.item())\n","            optimizer.step()\n","\n","            # Total correct predictions\n","            #predicted = torch.max(output.data,0)[1]\n","            predicted = torch.max(output.data, 1)[1] \n","            correct += (predicted == var_y_batch).sum()\n","            #print(correct)\n","            if batch_idx % 100 == 0:\n","                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n","                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.item(), float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))\n","    return lossvec\n","                "]},{"cell_type":"code","execution_count":70,"metadata":{"_uuid":"766f99fea9d2295443131e64e9bda28e1ea1efe5","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch : 0 [0/5376 (0%)]\tLoss: 2.308540\t Accuracy:11.500%\n","Epoch : 1 [0/5376 (0%)]\tLoss: 1.550054\t Accuracy:50.000%\n","Epoch : 2 [0/5376 (0%)]\tLoss: 1.245156\t Accuracy:61.500%\n","Epoch : 3 [0/5376 (0%)]\tLoss: 0.990328\t Accuracy:71.000%\n","Epoch : 4 [0/5376 (0%)]\tLoss: 0.767749\t Accuracy:78.000%\n","Epoch : 5 [0/5376 (0%)]\tLoss: 0.575144\t Accuracy:85.000%\n","Epoch : 6 [0/5376 (0%)]\tLoss: 0.471227\t Accuracy:86.500%\n","Epoch : 7 [0/5376 (0%)]\tLoss: 0.328446\t Accuracy:90.000%\n","Epoch : 8 [0/5376 (0%)]\tLoss: 0.271350\t Accuracy:94.500%\n","Epoch : 9 [0/5376 (0%)]\tLoss: 0.191229\t Accuracy:96.000%\n","Epoch : 10 [0/5376 (0%)]\tLoss: 0.179959\t Accuracy:95.000%\n","Epoch : 11 [0/5376 (0%)]\tLoss: 0.115611\t Accuracy:97.500%\n","Epoch : 12 [0/5376 (0%)]\tLoss: 0.114002\t Accuracy:97.000%\n","Epoch : 13 [0/5376 (0%)]\tLoss: 0.060140\t Accuracy:99.000%\n","Epoch : 14 [0/5376 (0%)]\tLoss: 0.094482\t Accuracy:98.000%\n","Epoch : 15 [0/5376 (0%)]\tLoss: 0.061409\t Accuracy:99.500%\n","Epoch : 16 [0/5376 (0%)]\tLoss: 0.078319\t Accuracy:98.500%\n","Epoch : 17 [0/5376 (0%)]\tLoss: 0.039126\t Accuracy:99.500%\n","Epoch : 18 [0/5376 (0%)]\tLoss: 0.019651\t Accuracy:100.000%\n","Epoch : 19 [0/5376 (0%)]\tLoss: 0.021344\t Accuracy:99.500%\n","Epoch : 20 [0/5376 (0%)]\tLoss: 0.020439\t Accuracy:100.000%\n","Epoch : 21 [0/5376 (0%)]\tLoss: 0.012718\t Accuracy:100.000%\n","Epoch : 22 [0/5376 (0%)]\tLoss: 0.010292\t Accuracy:100.000%\n","Epoch : 23 [0/5376 (0%)]\tLoss: 0.008644\t Accuracy:100.000%\n","Epoch : 24 [0/5376 (0%)]\tLoss: 0.007791\t Accuracy:100.000%\n","Epoch : 25 [0/5376 (0%)]\tLoss: 0.010800\t Accuracy:99.500%\n","Epoch : 26 [0/5376 (0%)]\tLoss: 0.005847\t Accuracy:100.000%\n","Epoch : 27 [0/5376 (0%)]\tLoss: 0.004963\t Accuracy:100.000%\n","Epoch : 28 [0/5376 (0%)]\tLoss: 0.004706\t Accuracy:100.000%\n","Epoch : 29 [0/5376 (0%)]\tLoss: 0.004237\t Accuracy:100.000%\n","Epoch : 30 [0/5376 (0%)]\tLoss: 0.003751\t Accuracy:100.000%\n","Epoch : 31 [0/5376 (0%)]\tLoss: 0.003731\t Accuracy:100.000%\n","Epoch : 32 [0/5376 (0%)]\tLoss: 0.003740\t Accuracy:100.000%\n","Epoch : 33 [0/5376 (0%)]\tLoss: 0.003527\t Accuracy:100.000%\n","Epoch : 34 [0/5376 (0%)]\tLoss: 0.003051\t Accuracy:100.000%\n","Epoch : 35 [0/5376 (0%)]\tLoss: 0.002641\t Accuracy:100.000%\n","Epoch : 36 [0/5376 (0%)]\tLoss: 0.002296\t Accuracy:100.000%\n","Epoch : 37 [0/5376 (0%)]\tLoss: 0.002025\t Accuracy:100.000%\n","Epoch : 38 [0/5376 (0%)]\tLoss: 0.001839\t Accuracy:100.000%\n","Epoch : 39 [0/5376 (0%)]\tLoss: 0.001698\t Accuracy:100.000%\n","Epoch : 40 [0/5376 (0%)]\tLoss: 0.001598\t Accuracy:100.000%\n","Epoch : 41 [0/5376 (0%)]\tLoss: 0.001507\t Accuracy:100.000%\n","Epoch : 42 [0/5376 (0%)]\tLoss: 0.001431\t Accuracy:100.000%\n","Epoch : 43 [0/5376 (0%)]\tLoss: 0.001355\t Accuracy:100.000%\n","Epoch : 44 [0/5376 (0%)]\tLoss: 0.001292\t Accuracy:100.000%\n","Epoch : 45 [0/5376 (0%)]\tLoss: 0.001226\t Accuracy:100.000%\n","Epoch : 46 [0/5376 (0%)]\tLoss: 0.001168\t Accuracy:100.000%\n","Epoch : 47 [0/5376 (0%)]\tLoss: 0.001114\t Accuracy:100.000%\n","Epoch : 48 [0/5376 (0%)]\tLoss: 0.001064\t Accuracy:100.000%\n","Epoch : 49 [0/5376 (0%)]\tLoss: 0.001013\t Accuracy:100.000%\n","Epoch : 50 [0/5376 (0%)]\tLoss: 0.000971\t Accuracy:100.000%\n","Epoch : 51 [0/5376 (0%)]\tLoss: 0.000929\t Accuracy:100.000%\n","Epoch : 52 [0/5376 (0%)]\tLoss: 0.000890\t Accuracy:100.000%\n","Epoch : 53 [0/5376 (0%)]\tLoss: 0.000850\t Accuracy:100.000%\n","Epoch : 54 [0/5376 (0%)]\tLoss: 0.000815\t Accuracy:100.000%\n","Epoch : 55 [0/5376 (0%)]\tLoss: 0.000781\t Accuracy:100.000%\n","Epoch : 56 [0/5376 (0%)]\tLoss: 0.000751\t Accuracy:100.000%\n","Epoch : 57 [0/5376 (0%)]\tLoss: 0.000718\t Accuracy:100.000%\n","Epoch : 58 [0/5376 (0%)]\tLoss: 0.000692\t Accuracy:100.000%\n","Epoch : 59 [0/5376 (0%)]\tLoss: 0.000665\t Accuracy:100.000%\n","Epoch : 60 [0/5376 (0%)]\tLoss: 0.000639\t Accuracy:100.000%\n","Epoch : 61 [0/5376 (0%)]\tLoss: 0.000616\t Accuracy:100.000%\n","Epoch : 62 [0/5376 (0%)]\tLoss: 0.000592\t Accuracy:100.000%\n","Epoch : 63 [0/5376 (0%)]\tLoss: 0.000570\t Accuracy:100.000%\n","Epoch : 64 [0/5376 (0%)]\tLoss: 0.000551\t Accuracy:100.000%\n","Epoch : 65 [0/5376 (0%)]\tLoss: 0.000530\t Accuracy:100.000%\n","Epoch : 66 [0/5376 (0%)]\tLoss: 0.000512\t Accuracy:100.000%\n","Epoch : 67 [0/5376 (0%)]\tLoss: 0.000494\t Accuracy:100.000%\n","Epoch : 68 [0/5376 (0%)]\tLoss: 0.000477\t Accuracy:100.000%\n","Epoch : 69 [0/5376 (0%)]\tLoss: 0.000461\t Accuracy:100.000%\n","Epoch : 70 [0/5376 (0%)]\tLoss: 0.000446\t Accuracy:100.000%\n","Epoch : 71 [0/5376 (0%)]\tLoss: 0.000431\t Accuracy:100.000%\n","Epoch : 72 [0/5376 (0%)]\tLoss: 0.000417\t Accuracy:100.000%\n","Epoch : 73 [0/5376 (0%)]\tLoss: 0.000404\t Accuracy:100.000%\n","Epoch : 74 [0/5376 (0%)]\tLoss: 0.000391\t Accuracy:100.000%\n","Epoch : 75 [0/5376 (0%)]\tLoss: 0.000378\t Accuracy:100.000%\n","Epoch : 76 [0/5376 (0%)]\tLoss: 0.000367\t Accuracy:100.000%\n","Epoch : 77 [0/5376 (0%)]\tLoss: 0.000355\t Accuracy:100.000%\n","Epoch : 78 [0/5376 (0%)]\tLoss: 0.000345\t Accuracy:100.000%\n","Epoch : 79 [0/5376 (0%)]\tLoss: 0.000334\t Accuracy:100.000%\n","Epoch : 80 [0/5376 (0%)]\tLoss: 0.000324\t Accuracy:100.000%\n","Epoch : 81 [0/5376 (0%)]\tLoss: 0.000314\t Accuracy:100.000%\n","Epoch : 82 [0/5376 (0%)]\tLoss: 0.000305\t Accuracy:100.000%\n","Epoch : 83 [0/5376 (0%)]\tLoss: 0.000296\t Accuracy:100.000%\n","Epoch : 84 [0/5376 (0%)]\tLoss: 0.000288\t Accuracy:100.000%\n","Epoch : 85 [0/5376 (0%)]\tLoss: 0.000280\t Accuracy:100.000%\n","Epoch : 86 [0/5376 (0%)]\tLoss: 0.000272\t Accuracy:100.000%\n","Epoch : 87 [0/5376 (0%)]\tLoss: 0.000264\t Accuracy:100.000%\n","Epoch : 88 [0/5376 (0%)]\tLoss: 0.000257\t Accuracy:100.000%\n","Epoch : 89 [0/5376 (0%)]\tLoss: 0.000249\t Accuracy:100.000%\n","Epoch : 90 [0/5376 (0%)]\tLoss: 0.000243\t Accuracy:100.000%\n","Epoch : 91 [0/5376 (0%)]\tLoss: 0.000236\t Accuracy:100.000%\n","Epoch : 92 [0/5376 (0%)]\tLoss: 0.000230\t Accuracy:100.000%\n","Epoch : 93 [0/5376 (0%)]\tLoss: 0.000223\t Accuracy:100.000%\n","Epoch : 94 [0/5376 (0%)]\tLoss: 0.000217\t Accuracy:100.000%\n","Epoch : 95 [0/5376 (0%)]\tLoss: 0.000212\t Accuracy:100.000%\n","Epoch : 96 [0/5376 (0%)]\tLoss: 0.000206\t Accuracy:100.000%\n","Epoch : 97 [0/5376 (0%)]\tLoss: 0.000201\t Accuracy:100.000%\n","Epoch : 98 [0/5376 (0%)]\tLoss: 0.000196\t Accuracy:100.000%\n","Epoch : 99 [0/5376 (0%)]\tLoss: 0.000191\t Accuracy:100.000%\n","Epoch : 100 [0/5376 (0%)]\tLoss: 0.000186\t Accuracy:100.000%\n","Epoch : 101 [0/5376 (0%)]\tLoss: 0.000181\t Accuracy:100.000%\n","Epoch : 102 [0/5376 (0%)]\tLoss: 0.000177\t Accuracy:100.000%\n","Epoch : 103 [0/5376 (0%)]\tLoss: 0.000172\t Accuracy:100.000%\n","Epoch : 104 [0/5376 (0%)]\tLoss: 0.000168\t Accuracy:100.000%\n","Epoch : 105 [0/5376 (0%)]\tLoss: 0.000164\t Accuracy:100.000%\n","Epoch : 106 [0/5376 (0%)]\tLoss: 0.000160\t Accuracy:100.000%\n","Epoch : 107 [0/5376 (0%)]\tLoss: 0.000156\t Accuracy:100.000%\n","Epoch : 108 [0/5376 (0%)]\tLoss: 0.000153\t Accuracy:100.000%\n","Epoch : 109 [0/5376 (0%)]\tLoss: 0.000149\t Accuracy:100.000%\n","Epoch : 110 [0/5376 (0%)]\tLoss: 0.000145\t Accuracy:100.000%\n","Epoch : 111 [0/5376 (0%)]\tLoss: 0.000142\t Accuracy:100.000%\n","Epoch : 112 [0/5376 (0%)]\tLoss: 0.000139\t Accuracy:100.000%\n","Epoch : 113 [0/5376 (0%)]\tLoss: 0.000135\t Accuracy:100.000%\n","Epoch : 114 [0/5376 (0%)]\tLoss: 0.000132\t Accuracy:100.000%\n","Epoch : 115 [0/5376 (0%)]\tLoss: 0.000129\t Accuracy:100.000%\n","Epoch : 116 [0/5376 (0%)]\tLoss: 0.000126\t Accuracy:100.000%\n","Epoch : 117 [0/5376 (0%)]\tLoss: 0.000123\t Accuracy:100.000%\n","Epoch : 118 [0/5376 (0%)]\tLoss: 0.000121\t Accuracy:100.000%\n","Epoch : 119 [0/5376 (0%)]\tLoss: 0.000118\t Accuracy:100.000%\n","Epoch : 120 [0/5376 (0%)]\tLoss: 0.000115\t Accuracy:100.000%\n","Epoch : 121 [0/5376 (0%)]\tLoss: 0.000113\t Accuracy:100.000%\n","Epoch : 122 [0/5376 (0%)]\tLoss: 0.000110\t Accuracy:100.000%\n","Epoch : 123 [0/5376 (0%)]\tLoss: 0.000108\t Accuracy:100.000%\n","Epoch : 124 [0/5376 (0%)]\tLoss: 0.000105\t Accuracy:100.000%\n","Epoch : 125 [0/5376 (0%)]\tLoss: 0.000103\t Accuracy:100.000%\n","Epoch : 126 [0/5376 (0%)]\tLoss: 0.000101\t Accuracy:100.000%\n","Epoch : 127 [0/5376 (0%)]\tLoss: 0.000099\t Accuracy:100.000%\n","Epoch : 128 [0/5376 (0%)]\tLoss: 0.000097\t Accuracy:100.000%\n","Epoch : 129 [0/5376 (0%)]\tLoss: 0.000095\t Accuracy:100.000%\n","Epoch : 130 [0/5376 (0%)]\tLoss: 0.000093\t Accuracy:100.000%\n","Epoch : 131 [0/5376 (0%)]\tLoss: 0.000091\t Accuracy:100.000%\n","Epoch : 132 [0/5376 (0%)]\tLoss: 0.000089\t Accuracy:100.000%\n","Epoch : 133 [0/5376 (0%)]\tLoss: 0.000087\t Accuracy:100.000%\n","Epoch : 134 [0/5376 (0%)]\tLoss: 0.000085\t Accuracy:100.000%\n","Epoch : 135 [0/5376 (0%)]\tLoss: 0.000083\t Accuracy:100.000%\n","Epoch : 136 [0/5376 (0%)]\tLoss: 0.000081\t Accuracy:100.000%\n","Epoch : 137 [0/5376 (0%)]\tLoss: 0.000080\t Accuracy:100.000%\n","Epoch : 138 [0/5376 (0%)]\tLoss: 0.000078\t Accuracy:100.000%\n","Epoch : 139 [0/5376 (0%)]\tLoss: 0.000077\t Accuracy:100.000%\n","Epoch : 140 [0/5376 (0%)]\tLoss: 0.000075\t Accuracy:100.000%\n","Epoch : 141 [0/5376 (0%)]\tLoss: 0.000074\t Accuracy:100.000%\n","Epoch : 142 [0/5376 (0%)]\tLoss: 0.000072\t Accuracy:100.000%\n","Epoch : 143 [0/5376 (0%)]\tLoss: 0.000071\t Accuracy:100.000%\n","Epoch : 144 [0/5376 (0%)]\tLoss: 0.000069\t Accuracy:100.000%\n","Epoch : 145 [0/5376 (0%)]\tLoss: 0.000068\t Accuracy:100.000%\n","Epoch : 146 [0/5376 (0%)]\tLoss: 0.000066\t Accuracy:100.000%\n","Epoch : 147 [0/5376 (0%)]\tLoss: 0.000065\t Accuracy:100.000%\n","Epoch : 148 [0/5376 (0%)]\tLoss: 0.000064\t Accuracy:100.000%\n","Epoch : 149 [0/5376 (0%)]\tLoss: 0.000063\t Accuracy:100.000%\n","Epoch : 150 [0/5376 (0%)]\tLoss: 0.000061\t Accuracy:100.000%\n","Epoch : 151 [0/5376 (0%)]\tLoss: 0.000060\t Accuracy:100.000%\n","Epoch : 152 [0/5376 (0%)]\tLoss: 0.000059\t Accuracy:100.000%\n","Epoch : 153 [0/5376 (0%)]\tLoss: 0.000058\t Accuracy:100.000%\n","Epoch : 154 [0/5376 (0%)]\tLoss: 0.000057\t Accuracy:100.000%\n","Epoch : 155 [0/5376 (0%)]\tLoss: 0.000056\t Accuracy:100.000%\n","Epoch : 156 [0/5376 (0%)]\tLoss: 0.000054\t Accuracy:100.000%\n","Epoch : 157 [0/5376 (0%)]\tLoss: 0.000053\t Accuracy:100.000%\n","Epoch : 158 [0/5376 (0%)]\tLoss: 0.000052\t Accuracy:100.000%\n","Epoch : 159 [0/5376 (0%)]\tLoss: 0.000051\t Accuracy:100.000%\n","Epoch : 160 [0/5376 (0%)]\tLoss: 0.000050\t Accuracy:100.000%\n","Epoch : 161 [0/5376 (0%)]\tLoss: 0.000049\t Accuracy:100.000%\n","Epoch : 162 [0/5376 (0%)]\tLoss: 0.000048\t Accuracy:100.000%\n","Epoch : 163 [0/5376 (0%)]\tLoss: 0.000048\t Accuracy:100.000%\n","Epoch : 164 [0/5376 (0%)]\tLoss: 0.000047\t Accuracy:100.000%\n","Epoch : 165 [0/5376 (0%)]\tLoss: 0.000046\t Accuracy:100.000%\n","Epoch : 166 [0/5376 (0%)]\tLoss: 0.000045\t Accuracy:100.000%\n","Epoch : 167 [0/5376 (0%)]\tLoss: 0.000044\t Accuracy:100.000%\n","Epoch : 168 [0/5376 (0%)]\tLoss: 0.000043\t Accuracy:100.000%\n","Epoch : 169 [0/5376 (0%)]\tLoss: 0.000042\t Accuracy:100.000%\n","Epoch : 170 [0/5376 (0%)]\tLoss: 0.000042\t Accuracy:100.000%\n","Epoch : 171 [0/5376 (0%)]\tLoss: 0.000041\t Accuracy:100.000%\n","Epoch : 172 [0/5376 (0%)]\tLoss: 0.000040\t Accuracy:100.000%\n","Epoch : 173 [0/5376 (0%)]\tLoss: 0.000039\t Accuracy:100.000%\n","Epoch : 174 [0/5376 (0%)]\tLoss: 0.000039\t Accuracy:100.000%\n","Epoch : 175 [0/5376 (0%)]\tLoss: 0.000038\t Accuracy:100.000%\n","Epoch : 176 [0/5376 (0%)]\tLoss: 0.000037\t Accuracy:100.000%\n","Epoch : 177 [0/5376 (0%)]\tLoss: 0.000036\t Accuracy:100.000%\n","Epoch : 178 [0/5376 (0%)]\tLoss: 0.000036\t Accuracy:100.000%\n","Epoch : 179 [0/5376 (0%)]\tLoss: 0.000035\t Accuracy:100.000%\n","Epoch : 180 [0/5376 (0%)]\tLoss: 0.000034\t Accuracy:100.000%\n","Epoch : 181 [0/5376 (0%)]\tLoss: 0.000034\t Accuracy:100.000%\n","Epoch : 182 [0/5376 (0%)]\tLoss: 0.000033\t Accuracy:100.000%\n","Epoch : 183 [0/5376 (0%)]\tLoss: 0.000033\t Accuracy:100.000%\n","Epoch : 184 [0/5376 (0%)]\tLoss: 0.000032\t Accuracy:100.000%\n","Epoch : 185 [0/5376 (0%)]\tLoss: 0.000031\t Accuracy:100.000%\n","Epoch : 186 [0/5376 (0%)]\tLoss: 0.000031\t Accuracy:100.000%\n","Epoch : 187 [0/5376 (0%)]\tLoss: 0.000030\t Accuracy:100.000%\n","Epoch : 188 [0/5376 (0%)]\tLoss: 0.000030\t Accuracy:100.000%\n","Epoch : 189 [0/5376 (0%)]\tLoss: 0.000029\t Accuracy:100.000%\n","Epoch : 190 [0/5376 (0%)]\tLoss: 0.000029\t Accuracy:100.000%\n","Epoch : 191 [0/5376 (0%)]\tLoss: 0.000028\t Accuracy:100.000%\n","Epoch : 192 [0/5376 (0%)]\tLoss: 0.000028\t Accuracy:100.000%\n","Epoch : 193 [0/5376 (0%)]\tLoss: 0.000027\t Accuracy:100.000%\n","Epoch : 194 [0/5376 (0%)]\tLoss: 0.000027\t Accuracy:100.000%\n","Epoch : 195 [0/5376 (0%)]\tLoss: 0.000026\t Accuracy:100.000%\n","Epoch : 196 [0/5376 (0%)]\tLoss: 0.000026\t Accuracy:100.000%\n","Epoch : 197 [0/5376 (0%)]\tLoss: 0.000025\t Accuracy:100.000%\n","Epoch : 198 [0/5376 (0%)]\tLoss: 0.000025\t Accuracy:100.000%\n","Epoch : 199 [0/5376 (0%)]\tLoss: 0.000024\t Accuracy:100.000%\n","Epoch : 200 [0/5376 (0%)]\tLoss: 0.000024\t Accuracy:100.000%\n","Epoch : 201 [0/5376 (0%)]\tLoss: 0.000024\t Accuracy:100.000%\n","Epoch : 202 [0/5376 (0%)]\tLoss: 0.000023\t Accuracy:100.000%\n","Epoch : 203 [0/5376 (0%)]\tLoss: 0.000023\t Accuracy:100.000%\n","Epoch : 204 [0/5376 (0%)]\tLoss: 0.000022\t Accuracy:100.000%\n","Epoch : 205 [0/5376 (0%)]\tLoss: 0.000022\t Accuracy:100.000%\n","Epoch : 206 [0/5376 (0%)]\tLoss: 0.000022\t Accuracy:100.000%\n","Epoch : 207 [0/5376 (0%)]\tLoss: 0.000021\t Accuracy:100.000%\n","Epoch : 208 [0/5376 (0%)]\tLoss: 0.000021\t Accuracy:100.000%\n","Epoch : 209 [0/5376 (0%)]\tLoss: 0.000020\t Accuracy:100.000%\n","Epoch : 210 [0/5376 (0%)]\tLoss: 0.000020\t Accuracy:100.000%\n","Epoch : 211 [0/5376 (0%)]\tLoss: 0.000020\t Accuracy:100.000%\n","Epoch : 212 [0/5376 (0%)]\tLoss: 0.000019\t Accuracy:100.000%\n","Epoch : 213 [0/5376 (0%)]\tLoss: 0.000019\t Accuracy:100.000%\n","Epoch : 214 [0/5376 (0%)]\tLoss: 0.000019\t Accuracy:100.000%\n","Epoch : 215 [0/5376 (0%)]\tLoss: 0.000018\t Accuracy:100.000%\n","Epoch : 216 [0/5376 (0%)]\tLoss: 0.000018\t Accuracy:100.000%\n","Epoch : 217 [0/5376 (0%)]\tLoss: 0.000018\t Accuracy:100.000%\n","Epoch : 218 [0/5376 (0%)]\tLoss: 0.000017\t Accuracy:100.000%\n","Epoch : 219 [0/5376 (0%)]\tLoss: 0.000017\t Accuracy:100.000%\n","Epoch : 220 [0/5376 (0%)]\tLoss: 0.000017\t Accuracy:100.000%\n","Epoch : 221 [0/5376 (0%)]\tLoss: 0.000017\t Accuracy:100.000%\n","Epoch : 222 [0/5376 (0%)]\tLoss: 0.000016\t Accuracy:100.000%\n","Epoch : 223 [0/5376 (0%)]\tLoss: 0.000016\t Accuracy:100.000%\n","Epoch : 224 [0/5376 (0%)]\tLoss: 0.000016\t Accuracy:100.000%\n","Epoch : 225 [0/5376 (0%)]\tLoss: 0.000015\t Accuracy:100.000%\n","Epoch : 226 [0/5376 (0%)]\tLoss: 0.000015\t Accuracy:100.000%\n","Epoch : 227 [0/5376 (0%)]\tLoss: 0.000015\t Accuracy:100.000%\n","Epoch : 228 [0/5376 (0%)]\tLoss: 0.000015\t Accuracy:100.000%\n","Epoch : 229 [0/5376 (0%)]\tLoss: 0.000014\t Accuracy:100.000%\n","Epoch : 230 [0/5376 (0%)]\tLoss: 0.000014\t Accuracy:100.000%\n","Epoch : 231 [0/5376 (0%)]\tLoss: 0.000014\t Accuracy:100.000%\n","Epoch : 232 [0/5376 (0%)]\tLoss: 0.000014\t Accuracy:100.000%\n","Epoch : 233 [0/5376 (0%)]\tLoss: 0.000014\t Accuracy:100.000%\n","Epoch : 234 [0/5376 (0%)]\tLoss: 0.000013\t Accuracy:100.000%\n","Epoch : 235 [0/5376 (0%)]\tLoss: 0.000013\t Accuracy:100.000%\n","Epoch : 236 [0/5376 (0%)]\tLoss: 0.000013\t Accuracy:100.000%\n","Epoch : 237 [0/5376 (0%)]\tLoss: 0.000013\t Accuracy:100.000%\n","Epoch : 238 [0/5376 (0%)]\tLoss: 0.000012\t Accuracy:100.000%\n","Epoch : 239 [0/5376 (0%)]\tLoss: 0.000012\t Accuracy:100.000%\n","Epoch : 240 [0/5376 (0%)]\tLoss: 0.000012\t Accuracy:100.000%\n","Epoch : 241 [0/5376 (0%)]\tLoss: 0.000012\t Accuracy:100.000%\n","Epoch : 242 [0/5376 (0%)]\tLoss: 0.000012\t Accuracy:100.000%\n","Epoch : 243 [0/5376 (0%)]\tLoss: 0.000011\t Accuracy:100.000%\n","Epoch : 244 [0/5376 (0%)]\tLoss: 0.000011\t Accuracy:100.000%\n","Epoch : 245 [0/5376 (0%)]\tLoss: 0.000011\t Accuracy:100.000%\n","Epoch : 246 [0/5376 (0%)]\tLoss: 0.000011\t Accuracy:100.000%\n","Epoch : 247 [0/5376 (0%)]\tLoss: 0.000011\t Accuracy:100.000%\n","Epoch : 248 [0/5376 (0%)]\tLoss: 0.000011\t Accuracy:100.000%\n","Epoch : 249 [0/5376 (0%)]\tLoss: 0.000010\t Accuracy:100.000%\n"]}],"source":["loss_vec = fit(mlp, train_loader)"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["plt.figure()\n","plt.plot(loss_vec)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"777eb068065662bea355507597b158eb8713b7f2"},"source":["## MLP Evaluation"]},{"cell_type":"code","execution_count":72,"metadata":{"_uuid":"a3aca2215610465fcccc58f1b98fc1d56096f364","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test accuracy:61.500% \n"]}],"source":["def evaluate(model):\n","#model = mlp\n","    predict_list = []\n","    correct = 0 \n","    for test_imgs, test_labels in test_loader:\n","        #print(test_imgs.shape)\n","        test_imgs = Variable(test_imgs).float()\n","        output = model(test_imgs)\n","        predicted = torch.max(output,1)[1]\n","        predict_list.append(predicted)\n","        correct += (predicted == test_labels).sum()\n","    print(\"Test accuracy:{:.3f}% \".format( float(correct)*100 / (len(test_loader)*BATCH_SIZE)))\n","predict = evaluate(mlp)"]},{"cell_type":"markdown","metadata":{"_uuid":"810c7b5869b3c704d5a5a7e28289133c27c5790c"},"source":["<center><h2>Convolutional Neural Network</h2></center>"]},{"cell_type":"markdown","metadata":{"_uuid":"0dd01423ecc2b3c46c42aaa033ad8556ba029dc9"},"source":["## Explanation\n","\n","To better understand convolutional neural network I recommend the great section on it here : http://cs231n.github.io/convolutional-networks/\n","\n","**Convolutional operation** : First let's clarify briefly how we can perform the convolutional operation on an image. For that we need to define a **kernel** which is a small matrix of size 5 \\* 5 for example. To perform the convolution operation, we just need to slide the kernel along the image horizontally and vertically and do the dot product of the kernel and the small portion of the image.\n","\n","**Pooling** : the convolutional operation give an output of the same size of the input image. To reduce the size of the image and thus reduce the number of paramers in the model we perform a Pooling operation. The pooling operation need a window size.. By sliding the window along the image, we compute the mean or the max of the portion of the image inside the window in case of MeanPooling or MaxPooling.\n","\n","**Stride** is the number of pixels to pass at a time when sliding the convolutional kernel.  \n","\n","**Padding** to preserve exactly the size of the input image, it is useful to add a zero padding on the border of the image. \n","\n","\n","**To remember** : What makes a CNN so interesting for images is that it is invariant by translation and for each convolutional layer we only need to store the kernels. Thus we can stack a lot of layers to learn deep features without having too much parameters that would make a model untrainnable. "]},{"cell_type":"markdown","metadata":{"_uuid":"9baaa52b6b9b6b8f8287a10fcc801606848fb726","trusted":true},"source":["## Data loader\n","\n","Since a CNN needs a image shape as input let's reshape our flatten images to real image"]},{"cell_type":"code","execution_count":73,"metadata":{"_uuid":"5e84347012cd6abc2f6f46299aab08b0c89563ee","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([5376, 1, 50, 50])\n","torch.Size([1344, 1, 50, 50])\n"]}],"source":["torch_X_train = torch_X_train.view(-1, 1,50,50).float()\n","torch_X_test = torch_X_test.view(-1,1,50,50).float()\n","print(torch_X_train.shape)\n","print(torch_X_test.shape)\n","\n","# Pytorch train and test sets\n","train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n","test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n","\n","# data loader\n","train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n","test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)\n"]},{"cell_type":"code","execution_count":74,"metadata":{"_uuid":"c669e5f557724154d9f4f5f8b5d0d6c9b676d551","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CNN(\n","  (conv1): Conv2d(1, 16, kernel_size=(2, 2), stride=(1, 1))\n","  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n","  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=5184, out_features=125, bias=True)\n","  (fc2): Linear(in_features=125, out_features=10, bias=True)\n",")\n","torch.Size([200, 1, 50, 50])\n","torch.Size([200, 10])\n"]}],"source":["class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 16, kernel_size=2)\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n","        self.conv3 = nn.Conv2d(32,64, kernel_size=5)\n","        self.fc1 = nn.Linear(9*9*64,125)\n","        self.fc2 =nn.Linear(125,10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = F.relu(F.max_pool2d(self.conv3(x),2))\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = x.view(-1,9*9*64)\n","        x = F.relu(self.fc1(x))\n","        x = F.dropout(x, training=self.training)\n","        x = self.fc2(x)\n","        return F.log_softmax(x, dim=1)\n"," \n","cnn = CNN()\n","print(cnn)\n","\n","it = iter(train_loader)\n","X_batch, y_batch = next(it)\n","print(X_batch.shape)\n","print(cnn.forward(X_batch).shape)"]},{"cell_type":"code","execution_count":75,"metadata":{"_uuid":"797cbe3cff05d8bdb42f3273a06839b18daf5266","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch : 0 [0/5376 (0%)]\tLoss: 2.300618\t Accuracy:11.000%\n","Epoch : 1 [0/5376 (0%)]\tLoss: 2.141267\t Accuracy:24.000%\n","Epoch : 2 [0/5376 (0%)]\tLoss: 1.844463\t Accuracy:37.000%\n","Epoch : 3 [0/5376 (0%)]\tLoss: 1.667828\t Accuracy:45.500%\n","Epoch : 4 [0/5376 (0%)]\tLoss: 1.402853\t Accuracy:53.000%\n","Epoch : 5 [0/5376 (0%)]\tLoss: 1.285275\t Accuracy:56.500%\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Blade\\OneDrive - University of Florida\\Coding\\CNN_Learning_2\\cnn-with-pytorch-for-mnist.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Blade/OneDrive%20-%20University%20of%20Florida/Coding/CNN_Learning_2/cnn-with-pytorch-for-mnist.ipynb#ch0000026?line=0'>1</a>\u001b[0m fit(cnn,train_loader)\n","\u001b[1;32mc:\\Users\\Blade\\OneDrive - University of Florida\\Coding\\CNN_Learning_2\\cnn-with-pytorch-for-mnist.ipynb Cell 15'\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, train_loader)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Blade/OneDrive%20-%20University%20of%20Florida/Coding/CNN_Learning_2/cnn-with-pytorch-for-mnist.ipynb#ch0000016?line=12'>13</a>\u001b[0m output \u001b[39m=\u001b[39m model(var_X_batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Blade/OneDrive%20-%20University%20of%20Florida/Coding/CNN_Learning_2/cnn-with-pytorch-for-mnist.ipynb#ch0000016?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m error(output, var_y_batch)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Blade/OneDrive%20-%20University%20of%20Florida/Coding/CNN_Learning_2/cnn-with-pytorch-for-mnist.ipynb#ch0000016?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Blade/OneDrive%20-%20University%20of%20Florida/Coding/CNN_Learning_2/cnn-with-pytorch-for-mnist.ipynb#ch0000016?line=15'>16</a>\u001b[0m lossvec\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Blade/OneDrive%20-%20University%20of%20Florida/Coding/CNN_Learning_2/cnn-with-pytorch-for-mnist.ipynb#ch0000016?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n","File \u001b[1;32m~\\Anaconda3\\envs\\hw\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n","File \u001b[1;32m~\\Anaconda3\\envs\\hw\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["fit(cnn,train_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"890dfc8d29ac70df919807b275b38112ca9297ab","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test accuracy:10.071% \n"]}],"source":["evaluate(cnn)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-2.2556, -2.3080, -2.3297,  ..., -2.3225, -2.3270, -2.3085],\n","        [-2.2557, -2.3084, -2.3296,  ..., -2.3227, -2.3274, -2.3082],\n","        [-2.2759, -2.3518, -2.3248,  ..., -2.3471, -2.3643, -2.2778],\n","        ...,\n","        [-2.2575, -2.3123, -2.3292,  ..., -2.3249, -2.3307, -2.3055],\n","        [-2.2759, -2.3518, -2.3248,  ..., -2.3471, -2.3643, -2.2778],\n","        [-2.2759, -2.3518, -2.3248,  ..., -2.3471, -2.3643, -2.2778]],\n","       grad_fn=<LogSoftmaxBackward0>)\n","torch.Size([200, 1, 300, 300])\n","torch.Size([200, 10])\n","torch.Size([200])\n","tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","        6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","        6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0,\n","        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6,\n","        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","        6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6,\n","        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 6, 6, 6, 6, 6, 6, 6,\n","        6, 6, 6, 6, 6, 6, 6, 6])\n"]}],"source":["output = cnn.forward(X_batch)\n","predicted = torch.max(output.data,1)[1]\n","print(output)\n","print(X_batch.shape)\n","print(output.shape)\n","print(predicted.shape)\n","print(predicted)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","        6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","        6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0,\n","        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6,\n","        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","        6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6,\n","        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n","        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 6, 6, 6, 6, 6, 6, 6,\n","        6, 6, 6, 6, 6, 6, 6, 6])\n"]}],"source":["predicted = torch.max(output.data,1)[1]\n","print(predicted)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
