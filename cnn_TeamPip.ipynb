{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook aims at discovering Convolutional Neural Network. We will see the theory behind it, and an implementation in Pytorch for hand-digits classification on MNIST dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "# History\n",
    "\n",
    "Contrary to what most people think, Neural Networks is quite an old concept. It was first introduced in 1957 under the name ***perceptron***. Peceptron is a 1-layer feed forward neural network. However the infrastructure and the algorthm around it was not good enough to allow large scale training. Later on in 1986, ***Multi Layer Perceptron (MLP)*** was introduced with the backpropagation algorithm in order to train a network with more than 1 layer. Thanks to this algorithm we are not able to train non-linear model which can learn high level abstract features. Then ***Convolutional Neural Network (CNN)*** has been introduced in order to learn better features and with the possibility to reduce the number of parameters to be trained. And now, here we are, in the ***Deep Learning era*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b3124a67ef42274e81ddbd3d46d4937a3e2dfaa5"
   },
   "source": [
    "    # Multi-Layer Perceptron\n",
    "\n",
    "The first thing to ask is : why do we needed Convolutional Neural Network in the first place... Well, let's see what happen when we train a Multi-Layer Perceptron to recognize hand-written digits. In Machine Learning we have our own \"Hello World\" which is the MNIST dataset. Let's see what this dataset is about and how a multi-layer perceptron will perform.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "46ade2a1807aadd90e577c496d77d3df507dad88"
   },
   "outputs": [],
   "source": [
    "import numpy as np # to handle matrix and data operation\n",
    "import pandas as pd # to read csv and handle dataframe\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "a87c9de979fb54874e3a047d40cc024a8b0f5e98"
   },
   "outputs": [],
   "source": [
    "data = np.load('Data/data_train.npy').T\n",
    "X_og = data.copy()\n",
    "#X_og= data.copy()/255\n",
    "y_og =np.load('Data/labels_corrected.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[161 162 162 ... 149 149 147]\n",
      " [162 162 162 ... 165 165 165]\n",
      " [195 195 195 ... 191 191 191]\n",
      " ...\n",
      " [195 195 195 ... 195 195 195]\n",
      " [158 157 157 ... 141 140 141]\n",
      " [149 149 150 ... 138 142 147]]\n"
     ]
    }
   ],
   "source": [
    "print(X_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 8 1 ... 1 3 3]\n"
     ]
    }
   ],
   "source": [
    "y = y_og.astype(int)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6720, 90000)\n",
      "(6720,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_og))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = 1-X_og.copy()\n",
    "def resize_func(input_data,new_width,new_height):#input the (1,90000) data\n",
    "    size1 = np.shape(input_data)[0]\n",
    "    size2 = int(size1**(0.5))\n",
    "    output = cv2.resize(input_data.reshape(size2,size2),(new_width,new_height))\n",
    "    return output\n",
    "\n",
    "\n",
    "def morph_ops(input_data,n_width,n_height): # morphological operations \n",
    "    dilate_kernel = np.ones((2,2),np.uint8)\n",
    "    x0 = 1-input_data\n",
    "    x0_1 = resize_func(x0,n_width,n_height)\n",
    "    x1 = x0_1.reshape(n_width,n_height)\n",
    "    x4 = np.clip(x1-np.mean(x1),0,1)\n",
    "    x5 = np.where(x1 < 0.35,0,x4)\n",
    "    x6 = cv2.dilate(x5,dilate_kernel,iterations=1)\n",
    "    x7 = np.where(x6<0.15,0,x6)\n",
    "    x8 = normalize(x7.reshape(-1,1)).reshape(n_width,n_height)\n",
    "    x9 = np.where(x8>0.3,1,x8)\n",
    "    x10 = x9.reshape(1,-1)\n",
    "    picture = x10\n",
    "    return picture # 1, n_width*n_height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morph_trans(input_data,n_width, n_height):\n",
    "    #image_low=resize_func(input_data,n_width,n_height)\n",
    "\n",
    "    image_gray=cv2.bitwise_not(input_data)\n",
    "    \n",
    "    ret,image_bw = cv2.threshold(image_gray,170,255,cv2.THRESH_BINARY)\n",
    "\n",
    "    #image_bw=cv2.adaptiveThreshold(image_gray,255,cv2.ADAPTIVE_THRESH_MEAN_C, \\\n",
    "    #                              cv2.THRESH_BINARY,20,-2)\n",
    "    \n",
    "    # create horizontal and vertical line \n",
    "    horizontal = np.copy(image_bw)\n",
    "    vertical = np.copy(image_bw)\n",
    "    # specify horizontal axis\n",
    "    cols = horizontal.shape[1]\n",
    "    hor_size = cols//8\n",
    "    \n",
    "    # create structural element\n",
    "    horizontal_struc =cv2.getStructuringElement(cv2.MORPH_RECT,\n",
    "                                                (hor_size,1))\n",
    "    horizontal =cv2.erode(horizontal,horizontal_struc, iterations=1)\n",
    "    horizontal = cv2.dilate(horizontal,horizontal_struc,iterations=1)\n",
    "    \n",
    "    # remove horizontal \n",
    "    cnts = cv2.findContours(horizontal,cv2.RETR_EXTERNAL,\n",
    "                            cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts)==2 else cnts[1]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # specify vertical \n",
    "    rows = vertical.shape[0]\n",
    "    ver_size = rows//36\n",
    "    \n",
    "    # create vertical structural element \n",
    "    vertical_struc = cv2.getStructuringElement(cv2.MORPH_RECT,(1,ver_size))\n",
    "    vertical =cv2.erode(vertical,vertical_struc)\n",
    "    vertical =cv2.dilate(vertical,vertical_struc)\n",
    "    \"\"\"\n",
    "    dilate_kernel = np.ones((6,1),np.uint8) \n",
    "    result=cv2.morphologyEx(image_bw,cv2.MORPH_OPEN,\n",
    "                                dilate_kernel,iterations=2)\n",
    "\n",
    "    \n",
    "    pic_dilate= cv2.dilate(image_bw, dilate_kernel, iterations=1)\n",
    "    #pic_dilate=cv2.erode(pic_dilate,dilate_kernel,iterations=2)\n",
    "    image_out=horizontal\n",
    "    return image_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##parameters \n",
    "# size after resize the original image \n",
    "n_width  = 64\n",
    "n_height = 64\n",
    "\n",
    "# split data\n",
    "test_data_size=0.2\n",
    "\n",
    "#batch and Epoch \n",
    "BATCH_SIZE =200\n",
    "Epoch = 500 \n",
    "\n",
    "\n",
    "# number of hidden layers \n",
    "h_layer_1 = 500 # hidden layer 1\n",
    "h_layer_2 = 100 # hidden layer 2\n",
    "h_layer_3 = 10  # hidden layer 3\n",
    "\n",
    "# learning rate \n",
    "learn_rate = 0.005\n",
    "\n",
    "# cnn kernel size \n",
    "kernel1 =2\n",
    "kernel2 =3\n",
    "kernel3 =5\n",
    "\n",
    "max_pool_kernel=2\n",
    "# convolution 1\n",
    "ch1_in=1\n",
    "ch1_out=16 \n",
    "stride1=1\n",
    "padding1=2\n",
    "\n",
    "#convolution 2\n",
    "ch2_in=ch1_out\n",
    "ch2_out= 32\n",
    "stride2=1\n",
    "padding2=2\n",
    "\n",
    "#convolution 3\n",
    "ch3_in=ch2_out\n",
    "ch3_out= 64\n",
    "stride3=1\n",
    "padding3=2\n",
    "\n",
    "#linear 1\n",
    "out_layer_size = 8*8\n",
    "linear1_in =out_layer_size*ch3_out\n",
    "linear1_out = 128\n",
    "\n",
    "#linear 2\n",
    "out_class =10\n",
    "linear2_in = linear1_out\n",
    "\n",
    "#dropout \n",
    "prob=0.2\n",
    "\n",
    "\n",
    "directory='Output_model/'\n",
    "filename ='CNN_'+'C_level1_'+'lr0_001'+'Ep_'+str(Epoch)+'B'+str(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.zeros((np.shape(X_og)[0],n_width*n_height))\n",
    "\n",
    "for ii in range(np.shape(X_new)[0]):\n",
    "    newrow = morph_ops(X_og[ii,:],n_width,n_height)\n",
    "    newrow = newrow.reshape(1,-1)\n",
    "    newrow = resize_func(newrow[0,:],n_width,n_height)\n",
    "    newrow = newrow.reshape(1,-1)\n",
    "    X_new[ii,:] = newrow\n",
    "X = X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d967962250>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 391\n",
    "'''\n",
    "fig=plt.figure(figsize=(20,10))\n",
    "\n",
    "for ii in range(index):\n",
    "    fig.add\n",
    "'''\n",
    "x1 = X[index,:].reshape(n_width,n_height)\n",
    "plt.imshow(x1,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "_uuid": "0b3d551a62defaadd37e681511ebc5fc70ac944d"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "96f0d5dbc90457eb091fb2e6ed68ce7c7bf6da0b"
   },
   "outputs": [],
   "source": [
    "\n",
    "torch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\n",
    "torch_y_train = torch.from_numpy(y_train).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# create feature and targets tensor for test set.\n",
    "torch_X_test = torch.from_numpy(X_test).type(torch.LongTensor)\n",
    "torch_y_test = torch.from_numpy(y_test).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
    "test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3624b779d746e6b5710711c7b1798363ecabafbb"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_width*n_height,h_layer_1)\n",
    "        self.linear2 = nn.Linear(h_layer_1,h_layer_2)\n",
    "        self.linear3 = nn.Linear(h_layer_2,out_class)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    " \n",
    "mlp = MLP()\n",
    "print(mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e471e447a9618edc7310bb15941054c30ea235a4"
   },
   "outputs": [],
   "source": [
    "def fit(model, train_loader):\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learn_rate)#,lr=0.001, betas=(0.9,0.999))\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    EPOCHS = Epoch\n",
    "    model.train()\n",
    "    lossvec = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        correct = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            lossvec.append(loss.item())\n",
    "            optimizer.step()\n",
    "\n",
    "            # Total correct predictions\n",
    "            #predicted = torch.max(output.data,0)[1]\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            #print(correct)\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.item(), float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
    "    return lossvec\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "766f99fea9d2295443131e64e9bda28e1ea1efe5"
   },
   "outputs": [],
   "source": [
    "loss_vec = fit(mlp, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_vec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "777eb068065662bea355507597b158eb8713b7f2"
   },
   "source": [
    "## MLP Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a3aca2215610465fcccc58f1b98fc1d56096f364"
   },
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "#model = mlp\n",
    "    predict_list = []\n",
    "    correct = 0 \n",
    "    for test_imgs, test_labels in test_loader:\n",
    "        #print(test_imgs.shape)\n",
    "        test_imgs = Variable(test_imgs).float()\n",
    "        output = model(test_imgs)\n",
    "        predicted = torch.max(output,1)[1]\n",
    "        predict_list.append(predicted)\n",
    "        correct += (predicted == test_labels).sum()\n",
    "    print(\"Test accuracy:{:.3f}% \".format( float(correct)*100 / (len(test_loader)*BATCH_SIZE)))\n",
    "predict = evaluate(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "810c7b5869b3c704d5a5a7e28289133c27c5790c"
   },
   "source": [
    "<center><h2>Convolutional Neural Network</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0dd01423ecc2b3c46c42aaa033ad8556ba029dc9"
   },
   "source": [
    "## Explanation\n",
    "\n",
    "To better understand convolutional neural network I recommend the great section on it here : http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "**Convolutional operation** : First let's clarify briefly how we can perform the convolutional operation on an image. For that we need to define a **kernel** which is a small matrix of size 5 \\* 5 for example. To perform the convolution operation, we just need to slide the kernel along the image horizontally and vertically and do the dot product of the kernel and the small portion of the image.\n",
    "\n",
    "**Pooling** : the convolutional operation give an output of the same size of the input image. To reduce the size of the image and thus reduce the number of paramers in the model we perform a Pooling operation. The pooling operation need a window size.. By sliding the window along the image, we compute the mean or the max of the portion of the image inside the window in case of MeanPooling or MaxPooling.\n",
    "\n",
    "**Stride** is the number of pixels to pass at a time when sliding the convolutional kernel.  \n",
    "\n",
    "**Padding** to preserve exactly the size of the input image, it is useful to add a zero padding on the border of the image. \n",
    "\n",
    "\n",
    "**To remember** : What makes a CNN so interesting for images is that it is invariant by translation and for each convolutional layer we only need to store the kernels. Thus we can stack a lot of layers to learn deep features without having too much parameters that would make a model untrainnable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9baaa52b6b9b6b8f8287a10fcc801606848fb726"
   },
   "source": [
    "## Data loader\n",
    "\n",
    "Since a CNN needs a image shape as input let's reshape our flatten images to real image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e84347012cd6abc2f6f46299aab08b0c89563ee"
   },
   "outputs": [],
   "source": [
    "torch_X_train = torch_X_train.view(-1, 1,n_width,n_height).float()\n",
    "torch_X_test = torch_X_test.view(-1,1,n_width,n_height).float()\n",
    "print(torch_X_train.shape)\n",
    "print(torch_X_test.shape)\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
    "test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(ch1_in, ch1_out, \n",
    "                               kernel_size=kernel1, \n",
    "                               stride=stride1,\n",
    "                              padding = padding1)\n",
    "        self.conv2 = nn.Conv2d(ch2_in, ch2_out, \n",
    "                               kernel_size=kernel2,\n",
    "                              stride = stride2,\n",
    "                              padding = padding2)\n",
    "        self.conv3 = nn.Conv2d(ch3_in, ch3_out, \n",
    "                               kernel_size=kernel3,\n",
    "                              stride =stride3,\n",
    "                              padding= padding3)\n",
    "        self.fc1 = nn.Linear(linear1_in,linear1_out)\n",
    "        self.fc2 =nn.Linear(linear2_in,out_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x),kernel_size=max_pool_kernel))\n",
    "        #x = F.dropout(x, p=prob, training=self.training)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=max_pool_kernel))\n",
    "        #x = F.dropout(x, p=prob, training=self.training)\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x),kernel_size=max_pool_kernel))\n",
    "        #x = F.dropout(x, p=prob, training=self.training)\n",
    "        x = x.view(x.size(0),-1)#linear1_in)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c669e5f557724154d9f4f5f8b5d0d6c9b676d551"
   },
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "class CNN_TA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_TA, self).__init__()\n",
    "        self.conv1 = nn.Sequential(               # input shape (1, n_width, n_height)\n",
    "            nn.Conv2d(\n",
    "                in_channels=ch1_in,               # input height\n",
    "                out_channels=ch1_out,             # n_filters\n",
    "                kernel_size=kernel1,              # filter size\n",
    "                stride=stride1,                   # filter movement/step\n",
    "                padding=padding1,                 # padding\n",
    "            ),                                    # output shape (16, n_width, n_height)\n",
    "            nn.ReLU(),                            # activation\n",
    "            nn.MaxPool2d(kernel_size=max_pool_kernel),    # choose max value in 2x2 area, output shape (16, n_width/kernel, n_height/kernel)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         # input shape (16, 14, 14)\n",
    "            nn.Conv2d(ch2_in, ch2_out, kernel2, stride2, padding2),     # output shape (32, 14, 14)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(kernel_size=max_pool_kernel),                # output shape (32, 7, 7)\n",
    "        )\n",
    "        self.out = nn.Linear(linear1_in, out_class)   # fully connected layer, output 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)           # flatten the output of conv2 to (BATCH_SIZE, 32 * 7 * 7)\n",
    "        output = self.out(x)\n",
    "        return output, x\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "#cnn = CNN_TA()\n",
    "print(cnn)\n",
    "\n",
    "it = iter(train_loader)\n",
    "X_batch, y_batch = next(it)\n",
    "print(X_batch.shape)\n",
    "print(cnn.forward(X_batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "797cbe3cff05d8bdb42f3273a06839b18daf5266"
   },
   "outputs": [],
   "source": [
    "fit(cnn,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "890dfc8d29ac70df919807b275b38112ca9297ab"
   },
   "outputs": [],
   "source": [
    "evaluate(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = cnn.forward(X_batch)\n",
    "#predicted = torch.max(output.data,1)[1]\n",
    "predicted = torch.max(output,1)[1].data.numpy()\n",
    "print(X_batch.shape)\n",
    "print(output.shape)\n",
    "print('Predicted number :',predicted[:10])\n",
    "print('Real Number :',y_batch[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save current model in an hdf5 file\n",
    "f_directory=directory+filename\n",
    "f_name=filename+'.hdf5'\n",
    "model_dir = Path(f_directory)\n",
    "model_dir.mkdir(parents = True, exist_ok = True)\n",
    "torch.save(cnn.state_dict(), model_dir.joinpath(f_name))\n",
    "\n",
    "\n",
    "\n",
    "# Uncomment the following line to load and print the saved model\n",
    "# model = torch.load(model_dir.joinpath('cnn_pytorch.hdf5'))\n",
    "# print(model)\n",
    "# Also visit https://pytorch.org/tutorials/beginner/saving_loading_models.html \n",
    "# for more details about saving and loading models in PyTorch.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
