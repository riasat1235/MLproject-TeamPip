{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook aims at discovering Convolutional Neural Network. We will see the theory behind it, and an implementation in Pytorch for hand-digits classification on MNIST dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "# History\n",
    "\n",
    "Contrary to what most people think, Neural Networks is quite an old concept. It was first introduced in 1957 under the name ***perceptron***. Peceptron is a 1-layer feed forward neural network. However the infrastructure and the algorthm around it was not good enough to allow large scale training. Later on in 1986, ***Multi Layer Perceptron (MLP)*** was introduced with the backpropagation algorithm in order to train a network with more than 1 layer. Thanks to this algorithm we are not able to train non-linear model which can learn high level abstract features. Then ***Convolutional Neural Network (CNN)*** has been introduced in order to learn better features and with the possibility to reduce the number of parameters to be trained. And now, here we are, in the ***Deep Learning era*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b3124a67ef42274e81ddbd3d46d4937a3e2dfaa5"
   },
   "source": [
    "    # Multi-Layer Perceptron\n",
    "\n",
    "The first thing to ask is : why do we needed Convolutional Neural Network in the first place... Well, let's see what happen when we train a Multi-Layer Perceptron to recognize hand-written digits. In Machine Learning we have our own \"Hello World\" which is the MNIST dataset. Let's see what this dataset is about and how a multi-layer perceptron will perform.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "46ade2a1807aadd90e577c496d77d3df507dad88"
   },
   "outputs": [],
   "source": [
    "import numpy as np # to handle matrix and data operation\n",
    "import pandas as pd # to read csv and handle dataframe\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "a87c9de979fb54874e3a047d40cc024a8b0f5e98"
   },
   "outputs": [],
   "source": [
    "data = np.load('Data/data_train.npy').T\n",
    "X_og = data.copy()/255\n",
    "y_og =np.load('Data/labels_corrected.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.63137255 0.63529412 0.63529412 ... 0.58431373 0.58431373 0.57647059]\n",
      " [0.63529412 0.63529412 0.63529412 ... 0.64705882 0.64705882 0.64705882]\n",
      " [0.76470588 0.76470588 0.76470588 ... 0.74901961 0.74901961 0.74901961]\n",
      " ...\n",
      " [0.76470588 0.76470588 0.76470588 ... 0.76470588 0.76470588 0.76470588]\n",
      " [0.61960784 0.61568627 0.61568627 ... 0.55294118 0.54901961 0.55294118]\n",
      " [0.58431373 0.58431373 0.58823529 ... 0.54117647 0.55686275 0.57647059]]\n"
     ]
    }
   ],
   "source": [
    "print(X_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 8 1 ... 1 3 3]\n"
     ]
    }
   ],
   "source": [
    "y = y_og.astype(int)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6720, 90000)\n",
      "(6720,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_og))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = 1-X_og.copy()\n",
    "def resize_func(input_data,new_width,new_height):#input the (1,90000) data\n",
    "    size1 = np.shape(input_data)[0]\n",
    "    size2 = int(size1**(0.5))\n",
    "    output = cv2.resize(input_data.reshape(size2,size2),(new_width,new_height))\n",
    "    return output\n",
    "\n",
    "\n",
    "def morph_ops(input_data,n_width,n_height): # morphological operations \n",
    "    dilate_kernel = np.ones((2,2),np.uint8)\n",
    "    x0 = 1-input_data\n",
    "    x0_1 = resize_func(x0,n_width,n_height)\n",
    "    x1 = x0_1.reshape(n_width,n_height)\n",
    "    x4 = np.clip(x1-np.mean(x1),0,1)\n",
    "    x5 = np.where(x1 < 0.35,0,x4)\n",
    "    dilate_kernel = np.ones((2,2),np.uint8)\n",
    "    x6 = cv2.dilate(x5,dilate_kernel,iterations=1)\n",
    "    x7 = np.where(x6<0.15,0,x6)\n",
    "    x8 = normalize(x7.reshape(-1,1)).reshape(n_width,n_height)\n",
    "    x9 = np.where(x8>0.3,1,x8)\n",
    "    x10 = x9.reshape(1,-1)\n",
    "    picture = x10\n",
    "    return picture # 1, n_width*n_height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##parameters \n",
    "# size after resize the original image \n",
    "n_width  =50 \n",
    "n_height =50\n",
    "\n",
    "# split data\n",
    "test_data_size=0.2\n",
    "\n",
    "#batch and Epoch \n",
    "BATCH_SIZE =200\n",
    "Epoch = 250 \n",
    "\n",
    "\n",
    "# number of hidden layers \n",
    "h_layer_1 = 500 # hidden layer 1\n",
    "h_layer_2 = 100 # hidden layer 2\n",
    "h_layer_3 = 10  # hidden layer 3\n",
    "\n",
    "# learning rate \n",
    "learn_rate = 0.001\n",
    "\n",
    "# cnn kernel size \n",
    "kernel1 =2\n",
    "kernel2 =3\n",
    "kernel3 =5\n",
    "\n",
    "max_pool_kernel=2\n",
    "# convolution 1\n",
    "ch1_in=1\n",
    "ch1_out=16 \n",
    "stride1=1\n",
    "padding1=2\n",
    "\n",
    "#convolution 2\n",
    "ch2_in=ch1_out\n",
    "ch2_out= 32\n",
    "stride2=1\n",
    "padding2=2\n",
    "\n",
    "#convolution 3\n",
    "ch3_in=ch2_out\n",
    "ch3_out= 64\n",
    "stride3=1\n",
    "padding3=2\n",
    "\n",
    "#linear 1\n",
    "out_layer_size = 7*7\n",
    "linear1_in =out_layer_size*ch3_out\n",
    "linear1_out = 125\n",
    "\n",
    "#linear 2\n",
    "out_class =10\n",
    "linear2_in = linear1_out\n",
    "\n",
    "#dropout \n",
    "prob=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.zeros((np.shape(X_og)[0],n_width*n_height))\n",
    "\n",
    "for ii in range(np.shape(X_new)[0]):\n",
    "    newrow = morph_ops(X_og[ii,:],n_width,n_height)\n",
    "    newrow = newrow.reshape(1,-1)\n",
    "    newrow = resize_func(newrow[0,:],n_width,n_height)\n",
    "    newrow = newrow.reshape(1,-1)\n",
    "    X_new[ii,:] = newrow\n",
    "X = X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f510e949a0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 391\n",
    "x1 = X[index,:].reshape(50,50)\n",
    "plt.imshow(x1,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "0b3d551a62defaadd37e681511ebc5fc70ac944d"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "96f0d5dbc90457eb091fb2e6ed68ce7c7bf6da0b"
   },
   "outputs": [],
   "source": [
    "\n",
    "torch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\n",
    "torch_y_train = torch.from_numpy(y_train).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# create feature and targets tensor for test set.\n",
    "torch_X_test = torch.from_numpy(X_test).type(torch.LongTensor)\n",
    "torch_y_test = torch.from_numpy(y_test).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
    "test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "3624b779d746e6b5710711c7b1798363ecabafbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=2500, out_features=500, bias=True)\n",
      "  (linear2): Linear(in_features=500, out_features=100, bias=True)\n",
      "  (linear3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_width*n_height,h_layer_1)\n",
    "        self.linear2 = nn.Linear(h_layer_1,h_layer_2)\n",
    "        self.linear3 = nn.Linear(h_layer_2,out_class)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    " \n",
    "mlp = MLP()\n",
    "print(mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "e471e447a9618edc7310bb15941054c30ea235a4"
   },
   "outputs": [],
   "source": [
    "def fit(model, train_loader):\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learn_rate)#,lr=0.001, betas=(0.9,0.999))\n",
    "    error = nn.CrossEntropyLoss()\n",
    "    EPOCHS = Epoch\n",
    "    model.train()\n",
    "    lossvec = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        correct = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss = error(output, var_y_batch)\n",
    "            loss.backward()\n",
    "            lossvec.append(loss.item())\n",
    "            optimizer.step()\n",
    "\n",
    "            # Total correct predictions\n",
    "            #predicted = torch.max(output.data,0)[1]\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            #print(correct)\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx / len(train_loader), loss.item(), float(correct*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
    "    return lossvec\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "766f99fea9d2295443131e64e9bda28e1ea1efe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/5376 (0%)]\tLoss: 2.304450\t Accuracy:11.000%\n",
      "Epoch : 1 [0/5376 (0%)]\tLoss: 1.508198\t Accuracy:49.500%\n",
      "Epoch : 2 [0/5376 (0%)]\tLoss: 1.195252\t Accuracy:62.000%\n",
      "Epoch : 3 [0/5376 (0%)]\tLoss: 0.966008\t Accuracy:69.000%\n",
      "Epoch : 4 [0/5376 (0%)]\tLoss: 0.774319\t Accuracy:76.500%\n",
      "Epoch : 5 [0/5376 (0%)]\tLoss: 0.603560\t Accuracy:81.500%\n",
      "Epoch : 6 [0/5376 (0%)]\tLoss: 0.420671\t Accuracy:88.500%\n",
      "Epoch : 7 [0/5376 (0%)]\tLoss: 0.301448\t Accuracy:93.500%\n",
      "Epoch : 8 [0/5376 (0%)]\tLoss: 0.258313\t Accuracy:92.500%\n",
      "Epoch : 9 [0/5376 (0%)]\tLoss: 0.213809\t Accuracy:94.500%\n",
      "Epoch : 10 [0/5376 (0%)]\tLoss: 0.143073\t Accuracy:97.000%\n",
      "Epoch : 11 [0/5376 (0%)]\tLoss: 0.125941\t Accuracy:97.000%\n",
      "Epoch : 12 [0/5376 (0%)]\tLoss: 0.083444\t Accuracy:99.000%\n",
      "Epoch : 13 [0/5376 (0%)]\tLoss: 0.075286\t Accuracy:99.500%\n",
      "Epoch : 14 [0/5376 (0%)]\tLoss: 0.063563\t Accuracy:99.500%\n",
      "Epoch : 15 [0/5376 (0%)]\tLoss: 0.046003\t Accuracy:100.000%\n",
      "Epoch : 16 [0/5376 (0%)]\tLoss: 0.035477\t Accuracy:100.000%\n",
      "Epoch : 17 [0/5376 (0%)]\tLoss: 0.027033\t Accuracy:100.000%\n",
      "Epoch : 18 [0/5376 (0%)]\tLoss: 0.025903\t Accuracy:100.000%\n",
      "Epoch : 19 [0/5376 (0%)]\tLoss: 0.035116\t Accuracy:99.000%\n",
      "Epoch : 20 [0/5376 (0%)]\tLoss: 0.021674\t Accuracy:100.000%\n",
      "Epoch : 21 [0/5376 (0%)]\tLoss: 0.019714\t Accuracy:100.000%\n",
      "Epoch : 22 [0/5376 (0%)]\tLoss: 0.014780\t Accuracy:100.000%\n",
      "Epoch : 23 [0/5376 (0%)]\tLoss: 0.012469\t Accuracy:100.000%\n",
      "Epoch : 24 [0/5376 (0%)]\tLoss: 0.009776\t Accuracy:100.000%\n",
      "Epoch : 25 [0/5376 (0%)]\tLoss: 0.038427\t Accuracy:99.500%\n",
      "Epoch : 26 [0/5376 (0%)]\tLoss: 0.008719\t Accuracy:100.000%\n",
      "Epoch : 27 [0/5376 (0%)]\tLoss: 0.009244\t Accuracy:100.000%\n",
      "Epoch : 28 [0/5376 (0%)]\tLoss: 0.006567\t Accuracy:100.000%\n",
      "Epoch : 29 [0/5376 (0%)]\tLoss: 0.006886\t Accuracy:100.000%\n",
      "Epoch : 30 [0/5376 (0%)]\tLoss: 0.005596\t Accuracy:100.000%\n",
      "Epoch : 31 [0/5376 (0%)]\tLoss: 0.005611\t Accuracy:100.000%\n",
      "Epoch : 32 [0/5376 (0%)]\tLoss: 0.005452\t Accuracy:100.000%\n",
      "Epoch : 33 [0/5376 (0%)]\tLoss: 0.004989\t Accuracy:100.000%\n",
      "Epoch : 34 [0/5376 (0%)]\tLoss: 0.004083\t Accuracy:100.000%\n",
      "Epoch : 35 [0/5376 (0%)]\tLoss: 0.003364\t Accuracy:100.000%\n",
      "Epoch : 36 [0/5376 (0%)]\tLoss: 0.002969\t Accuracy:100.000%\n",
      "Epoch : 37 [0/5376 (0%)]\tLoss: 0.002708\t Accuracy:100.000%\n",
      "Epoch : 38 [0/5376 (0%)]\tLoss: 0.002474\t Accuracy:100.000%\n",
      "Epoch : 39 [0/5376 (0%)]\tLoss: 0.002282\t Accuracy:100.000%\n",
      "Epoch : 40 [0/5376 (0%)]\tLoss: 0.002107\t Accuracy:100.000%\n",
      "Epoch : 41 [0/5376 (0%)]\tLoss: 0.001952\t Accuracy:100.000%\n",
      "Epoch : 42 [0/5376 (0%)]\tLoss: 0.001815\t Accuracy:100.000%\n",
      "Epoch : 43 [0/5376 (0%)]\tLoss: 0.001690\t Accuracy:100.000%\n",
      "Epoch : 44 [0/5376 (0%)]\tLoss: 0.001581\t Accuracy:100.000%\n",
      "Epoch : 45 [0/5376 (0%)]\tLoss: 0.001481\t Accuracy:100.000%\n",
      "Epoch : 46 [0/5376 (0%)]\tLoss: 0.001392\t Accuracy:100.000%\n",
      "Epoch : 47 [0/5376 (0%)]\tLoss: 0.001311\t Accuracy:100.000%\n",
      "Epoch : 48 [0/5376 (0%)]\tLoss: 0.001236\t Accuracy:100.000%\n",
      "Epoch : 49 [0/5376 (0%)]\tLoss: 0.001167\t Accuracy:100.000%\n",
      "Epoch : 50 [0/5376 (0%)]\tLoss: 0.001105\t Accuracy:100.000%\n",
      "Epoch : 51 [0/5376 (0%)]\tLoss: 0.001048\t Accuracy:100.000%\n",
      "Epoch : 52 [0/5376 (0%)]\tLoss: 0.000995\t Accuracy:100.000%\n",
      "Epoch : 53 [0/5376 (0%)]\tLoss: 0.000945\t Accuracy:100.000%\n",
      "Epoch : 54 [0/5376 (0%)]\tLoss: 0.000899\t Accuracy:100.000%\n",
      "Epoch : 55 [0/5376 (0%)]\tLoss: 0.000856\t Accuracy:100.000%\n",
      "Epoch : 56 [0/5376 (0%)]\tLoss: 0.000817\t Accuracy:100.000%\n",
      "Epoch : 57 [0/5376 (0%)]\tLoss: 0.000779\t Accuracy:100.000%\n",
      "Epoch : 58 [0/5376 (0%)]\tLoss: 0.000745\t Accuracy:100.000%\n",
      "Epoch : 59 [0/5376 (0%)]\tLoss: 0.000712\t Accuracy:100.000%\n",
      "Epoch : 60 [0/5376 (0%)]\tLoss: 0.000682\t Accuracy:100.000%\n",
      "Epoch : 61 [0/5376 (0%)]\tLoss: 0.000654\t Accuracy:100.000%\n",
      "Epoch : 62 [0/5376 (0%)]\tLoss: 0.000627\t Accuracy:100.000%\n",
      "Epoch : 63 [0/5376 (0%)]\tLoss: 0.000601\t Accuracy:100.000%\n",
      "Epoch : 64 [0/5376 (0%)]\tLoss: 0.000578\t Accuracy:100.000%\n",
      "Epoch : 65 [0/5376 (0%)]\tLoss: 0.000555\t Accuracy:100.000%\n",
      "Epoch : 66 [0/5376 (0%)]\tLoss: 0.000534\t Accuracy:100.000%\n",
      "Epoch : 67 [0/5376 (0%)]\tLoss: 0.000514\t Accuracy:100.000%\n",
      "Epoch : 68 [0/5376 (0%)]\tLoss: 0.000495\t Accuracy:100.000%\n",
      "Epoch : 69 [0/5376 (0%)]\tLoss: 0.000476\t Accuracy:100.000%\n",
      "Epoch : 70 [0/5376 (0%)]\tLoss: 0.000459\t Accuracy:100.000%\n",
      "Epoch : 71 [0/5376 (0%)]\tLoss: 0.000443\t Accuracy:100.000%\n",
      "Epoch : 72 [0/5376 (0%)]\tLoss: 0.000427\t Accuracy:100.000%\n",
      "Epoch : 73 [0/5376 (0%)]\tLoss: 0.000412\t Accuracy:100.000%\n",
      "Epoch : 74 [0/5376 (0%)]\tLoss: 0.000397\t Accuracy:100.000%\n",
      "Epoch : 75 [0/5376 (0%)]\tLoss: 0.000384\t Accuracy:100.000%\n",
      "Epoch : 76 [0/5376 (0%)]\tLoss: 0.000371\t Accuracy:100.000%\n",
      "Epoch : 77 [0/5376 (0%)]\tLoss: 0.000358\t Accuracy:100.000%\n",
      "Epoch : 78 [0/5376 (0%)]\tLoss: 0.000346\t Accuracy:100.000%\n",
      "Epoch : 79 [0/5376 (0%)]\tLoss: 0.000335\t Accuracy:100.000%\n",
      "Epoch : 80 [0/5376 (0%)]\tLoss: 0.000324\t Accuracy:100.000%\n",
      "Epoch : 81 [0/5376 (0%)]\tLoss: 0.000313\t Accuracy:100.000%\n",
      "Epoch : 82 [0/5376 (0%)]\tLoss: 0.000303\t Accuracy:100.000%\n",
      "Epoch : 83 [0/5376 (0%)]\tLoss: 0.000294\t Accuracy:100.000%\n",
      "Epoch : 84 [0/5376 (0%)]\tLoss: 0.000285\t Accuracy:100.000%\n",
      "Epoch : 85 [0/5376 (0%)]\tLoss: 0.000276\t Accuracy:100.000%\n",
      "Epoch : 86 [0/5376 (0%)]\tLoss: 0.000268\t Accuracy:100.000%\n",
      "Epoch : 87 [0/5376 (0%)]\tLoss: 0.000260\t Accuracy:100.000%\n",
      "Epoch : 88 [0/5376 (0%)]\tLoss: 0.000253\t Accuracy:100.000%\n",
      "Epoch : 89 [0/5376 (0%)]\tLoss: 0.000245\t Accuracy:100.000%\n",
      "Epoch : 90 [0/5376 (0%)]\tLoss: 0.000238\t Accuracy:100.000%\n",
      "Epoch : 91 [0/5376 (0%)]\tLoss: 0.000231\t Accuracy:100.000%\n",
      "Epoch : 92 [0/5376 (0%)]\tLoss: 0.000224\t Accuracy:100.000%\n",
      "Epoch : 93 [0/5376 (0%)]\tLoss: 0.000218\t Accuracy:100.000%\n",
      "Epoch : 94 [0/5376 (0%)]\tLoss: 0.000212\t Accuracy:100.000%\n",
      "Epoch : 95 [0/5376 (0%)]\tLoss: 0.000206\t Accuracy:100.000%\n",
      "Epoch : 96 [0/5376 (0%)]\tLoss: 0.000201\t Accuracy:100.000%\n",
      "Epoch : 97 [0/5376 (0%)]\tLoss: 0.000195\t Accuracy:100.000%\n",
      "Epoch : 98 [0/5376 (0%)]\tLoss: 0.000190\t Accuracy:100.000%\n",
      "Epoch : 99 [0/5376 (0%)]\tLoss: 0.000185\t Accuracy:100.000%\n",
      "Epoch : 100 [0/5376 (0%)]\tLoss: 0.000180\t Accuracy:100.000%\n",
      "Epoch : 101 [0/5376 (0%)]\tLoss: 0.000175\t Accuracy:100.000%\n",
      "Epoch : 102 [0/5376 (0%)]\tLoss: 0.000171\t Accuracy:100.000%\n",
      "Epoch : 103 [0/5376 (0%)]\tLoss: 0.000166\t Accuracy:100.000%\n",
      "Epoch : 104 [0/5376 (0%)]\tLoss: 0.000162\t Accuracy:100.000%\n",
      "Epoch : 105 [0/5376 (0%)]\tLoss: 0.000158\t Accuracy:100.000%\n",
      "Epoch : 106 [0/5376 (0%)]\tLoss: 0.000154\t Accuracy:100.000%\n",
      "Epoch : 107 [0/5376 (0%)]\tLoss: 0.000150\t Accuracy:100.000%\n",
      "Epoch : 108 [0/5376 (0%)]\tLoss: 0.000146\t Accuracy:100.000%\n",
      "Epoch : 109 [0/5376 (0%)]\tLoss: 0.000143\t Accuracy:100.000%\n",
      "Epoch : 110 [0/5376 (0%)]\tLoss: 0.000139\t Accuracy:100.000%\n",
      "Epoch : 111 [0/5376 (0%)]\tLoss: 0.000136\t Accuracy:100.000%\n",
      "Epoch : 112 [0/5376 (0%)]\tLoss: 0.000132\t Accuracy:100.000%\n",
      "Epoch : 113 [0/5376 (0%)]\tLoss: 0.000129\t Accuracy:100.000%\n",
      "Epoch : 114 [0/5376 (0%)]\tLoss: 0.000126\t Accuracy:100.000%\n",
      "Epoch : 115 [0/5376 (0%)]\tLoss: 0.000123\t Accuracy:100.000%\n",
      "Epoch : 116 [0/5376 (0%)]\tLoss: 0.000120\t Accuracy:100.000%\n",
      "Epoch : 117 [0/5376 (0%)]\tLoss: 0.000117\t Accuracy:100.000%\n",
      "Epoch : 118 [0/5376 (0%)]\tLoss: 0.000115\t Accuracy:100.000%\n",
      "Epoch : 119 [0/5376 (0%)]\tLoss: 0.000112\t Accuracy:100.000%\n",
      "Epoch : 120 [0/5376 (0%)]\tLoss: 0.000109\t Accuracy:100.000%\n",
      "Epoch : 121 [0/5376 (0%)]\tLoss: 0.000107\t Accuracy:100.000%\n",
      "Epoch : 122 [0/5376 (0%)]\tLoss: 0.000104\t Accuracy:100.000%\n",
      "Epoch : 123 [0/5376 (0%)]\tLoss: 0.000102\t Accuracy:100.000%\n",
      "Epoch : 124 [0/5376 (0%)]\tLoss: 0.000100\t Accuracy:100.000%\n",
      "Epoch : 125 [0/5376 (0%)]\tLoss: 0.000098\t Accuracy:100.000%\n",
      "Epoch : 126 [0/5376 (0%)]\tLoss: 0.000095\t Accuracy:100.000%\n",
      "Epoch : 127 [0/5376 (0%)]\tLoss: 0.000093\t Accuracy:100.000%\n",
      "Epoch : 128 [0/5376 (0%)]\tLoss: 0.000091\t Accuracy:100.000%\n",
      "Epoch : 129 [0/5376 (0%)]\tLoss: 0.000089\t Accuracy:100.000%\n",
      "Epoch : 130 [0/5376 (0%)]\tLoss: 0.000087\t Accuracy:100.000%\n",
      "Epoch : 131 [0/5376 (0%)]\tLoss: 0.000085\t Accuracy:100.000%\n",
      "Epoch : 132 [0/5376 (0%)]\tLoss: 0.000083\t Accuracy:100.000%\n",
      "Epoch : 133 [0/5376 (0%)]\tLoss: 0.000082\t Accuracy:100.000%\n",
      "Epoch : 134 [0/5376 (0%)]\tLoss: 0.000080\t Accuracy:100.000%\n",
      "Epoch : 135 [0/5376 (0%)]\tLoss: 0.000078\t Accuracy:100.000%\n",
      "Epoch : 136 [0/5376 (0%)]\tLoss: 0.000076\t Accuracy:100.000%\n",
      "Epoch : 137 [0/5376 (0%)]\tLoss: 0.000075\t Accuracy:100.000%\n",
      "Epoch : 138 [0/5376 (0%)]\tLoss: 0.000073\t Accuracy:100.000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 139 [0/5376 (0%)]\tLoss: 0.000072\t Accuracy:100.000%\n",
      "Epoch : 140 [0/5376 (0%)]\tLoss: 0.000070\t Accuracy:100.000%\n",
      "Epoch : 141 [0/5376 (0%)]\tLoss: 0.000069\t Accuracy:100.000%\n",
      "Epoch : 142 [0/5376 (0%)]\tLoss: 0.000068\t Accuracy:100.000%\n",
      "Epoch : 143 [0/5376 (0%)]\tLoss: 0.000066\t Accuracy:100.000%\n",
      "Epoch : 144 [0/5376 (0%)]\tLoss: 0.000065\t Accuracy:100.000%\n",
      "Epoch : 145 [0/5376 (0%)]\tLoss: 0.000063\t Accuracy:100.000%\n",
      "Epoch : 146 [0/5376 (0%)]\tLoss: 0.000062\t Accuracy:100.000%\n",
      "Epoch : 147 [0/5376 (0%)]\tLoss: 0.000061\t Accuracy:100.000%\n",
      "Epoch : 148 [0/5376 (0%)]\tLoss: 0.000060\t Accuracy:100.000%\n",
      "Epoch : 149 [0/5376 (0%)]\tLoss: 0.000058\t Accuracy:100.000%\n",
      "Epoch : 150 [0/5376 (0%)]\tLoss: 0.000057\t Accuracy:100.000%\n",
      "Epoch : 151 [0/5376 (0%)]\tLoss: 0.000056\t Accuracy:100.000%\n",
      "Epoch : 152 [0/5376 (0%)]\tLoss: 0.000055\t Accuracy:100.000%\n",
      "Epoch : 153 [0/5376 (0%)]\tLoss: 0.000054\t Accuracy:100.000%\n",
      "Epoch : 154 [0/5376 (0%)]\tLoss: 0.000053\t Accuracy:100.000%\n",
      "Epoch : 155 [0/5376 (0%)]\tLoss: 0.000052\t Accuracy:100.000%\n",
      "Epoch : 156 [0/5376 (0%)]\tLoss: 0.000051\t Accuracy:100.000%\n",
      "Epoch : 157 [0/5376 (0%)]\tLoss: 0.000050\t Accuracy:100.000%\n",
      "Epoch : 158 [0/5376 (0%)]\tLoss: 0.000049\t Accuracy:100.000%\n",
      "Epoch : 159 [0/5376 (0%)]\tLoss: 0.000048\t Accuracy:100.000%\n",
      "Epoch : 160 [0/5376 (0%)]\tLoss: 0.000047\t Accuracy:100.000%\n",
      "Epoch : 161 [0/5376 (0%)]\tLoss: 0.000046\t Accuracy:100.000%\n",
      "Epoch : 162 [0/5376 (0%)]\tLoss: 0.000045\t Accuracy:100.000%\n",
      "Epoch : 163 [0/5376 (0%)]\tLoss: 0.000044\t Accuracy:100.000%\n",
      "Epoch : 164 [0/5376 (0%)]\tLoss: 0.000043\t Accuracy:100.000%\n",
      "Epoch : 165 [0/5376 (0%)]\tLoss: 0.000043\t Accuracy:100.000%\n",
      "Epoch : 166 [0/5376 (0%)]\tLoss: 0.000042\t Accuracy:100.000%\n",
      "Epoch : 167 [0/5376 (0%)]\tLoss: 0.000041\t Accuracy:100.000%\n",
      "Epoch : 168 [0/5376 (0%)]\tLoss: 0.000040\t Accuracy:100.000%\n",
      "Epoch : 169 [0/5376 (0%)]\tLoss: 0.000039\t Accuracy:100.000%\n",
      "Epoch : 170 [0/5376 (0%)]\tLoss: 0.000039\t Accuracy:100.000%\n",
      "Epoch : 171 [0/5376 (0%)]\tLoss: 0.000038\t Accuracy:100.000%\n",
      "Epoch : 172 [0/5376 (0%)]\tLoss: 0.000037\t Accuracy:100.000%\n",
      "Epoch : 173 [0/5376 (0%)]\tLoss: 0.000037\t Accuracy:100.000%\n",
      "Epoch : 174 [0/5376 (0%)]\tLoss: 0.000036\t Accuracy:100.000%\n",
      "Epoch : 175 [0/5376 (0%)]\tLoss: 0.000035\t Accuracy:100.000%\n",
      "Epoch : 176 [0/5376 (0%)]\tLoss: 0.000035\t Accuracy:100.000%\n",
      "Epoch : 177 [0/5376 (0%)]\tLoss: 0.000034\t Accuracy:100.000%\n",
      "Epoch : 178 [0/5376 (0%)]\tLoss: 0.000033\t Accuracy:100.000%\n",
      "Epoch : 179 [0/5376 (0%)]\tLoss: 0.000033\t Accuracy:100.000%\n",
      "Epoch : 180 [0/5376 (0%)]\tLoss: 0.000032\t Accuracy:100.000%\n",
      "Epoch : 181 [0/5376 (0%)]\tLoss: 0.000032\t Accuracy:100.000%\n",
      "Epoch : 182 [0/5376 (0%)]\tLoss: 0.000031\t Accuracy:100.000%\n",
      "Epoch : 183 [0/5376 (0%)]\tLoss: 0.000030\t Accuracy:100.000%\n",
      "Epoch : 184 [0/5376 (0%)]\tLoss: 0.000030\t Accuracy:100.000%\n",
      "Epoch : 185 [0/5376 (0%)]\tLoss: 0.000029\t Accuracy:100.000%\n",
      "Epoch : 186 [0/5376 (0%)]\tLoss: 0.000029\t Accuracy:100.000%\n",
      "Epoch : 187 [0/5376 (0%)]\tLoss: 0.000028\t Accuracy:100.000%\n",
      "Epoch : 188 [0/5376 (0%)]\tLoss: 0.000028\t Accuracy:100.000%\n",
      "Epoch : 189 [0/5376 (0%)]\tLoss: 0.000027\t Accuracy:100.000%\n",
      "Epoch : 190 [0/5376 (0%)]\tLoss: 0.000027\t Accuracy:100.000%\n",
      "Epoch : 191 [0/5376 (0%)]\tLoss: 0.000026\t Accuracy:100.000%\n",
      "Epoch : 192 [0/5376 (0%)]\tLoss: 0.000026\t Accuracy:100.000%\n",
      "Epoch : 193 [0/5376 (0%)]\tLoss: 0.000025\t Accuracy:100.000%\n",
      "Epoch : 194 [0/5376 (0%)]\tLoss: 0.000025\t Accuracy:100.000%\n",
      "Epoch : 195 [0/5376 (0%)]\tLoss: 0.000024\t Accuracy:100.000%\n",
      "Epoch : 196 [0/5376 (0%)]\tLoss: 0.000024\t Accuracy:100.000%\n",
      "Epoch : 197 [0/5376 (0%)]\tLoss: 0.000024\t Accuracy:100.000%\n",
      "Epoch : 198 [0/5376 (0%)]\tLoss: 0.000023\t Accuracy:100.000%\n",
      "Epoch : 199 [0/5376 (0%)]\tLoss: 0.000023\t Accuracy:100.000%\n",
      "Epoch : 200 [0/5376 (0%)]\tLoss: 0.000022\t Accuracy:100.000%\n",
      "Epoch : 201 [0/5376 (0%)]\tLoss: 0.000022\t Accuracy:100.000%\n",
      "Epoch : 202 [0/5376 (0%)]\tLoss: 0.000022\t Accuracy:100.000%\n",
      "Epoch : 203 [0/5376 (0%)]\tLoss: 0.000021\t Accuracy:100.000%\n",
      "Epoch : 204 [0/5376 (0%)]\tLoss: 0.000021\t Accuracy:100.000%\n",
      "Epoch : 205 [0/5376 (0%)]\tLoss: 0.000021\t Accuracy:100.000%\n",
      "Epoch : 206 [0/5376 (0%)]\tLoss: 0.000020\t Accuracy:100.000%\n",
      "Epoch : 207 [0/5376 (0%)]\tLoss: 0.000020\t Accuracy:100.000%\n",
      "Epoch : 208 [0/5376 (0%)]\tLoss: 0.000019\t Accuracy:100.000%\n",
      "Epoch : 209 [0/5376 (0%)]\tLoss: 0.000019\t Accuracy:100.000%\n",
      "Epoch : 210 [0/5376 (0%)]\tLoss: 0.000019\t Accuracy:100.000%\n",
      "Epoch : 211 [0/5376 (0%)]\tLoss: 0.000019\t Accuracy:100.000%\n",
      "Epoch : 212 [0/5376 (0%)]\tLoss: 0.000018\t Accuracy:100.000%\n",
      "Epoch : 213 [0/5376 (0%)]\tLoss: 0.000018\t Accuracy:100.000%\n",
      "Epoch : 214 [0/5376 (0%)]\tLoss: 0.000018\t Accuracy:100.000%\n",
      "Epoch : 215 [0/5376 (0%)]\tLoss: 0.000017\t Accuracy:100.000%\n",
      "Epoch : 216 [0/5376 (0%)]\tLoss: 0.000017\t Accuracy:100.000%\n",
      "Epoch : 217 [0/5376 (0%)]\tLoss: 0.000017\t Accuracy:100.000%\n",
      "Epoch : 218 [0/5376 (0%)]\tLoss: 0.000016\t Accuracy:100.000%\n",
      "Epoch : 219 [0/5376 (0%)]\tLoss: 0.000016\t Accuracy:100.000%\n",
      "Epoch : 220 [0/5376 (0%)]\tLoss: 0.000016\t Accuracy:100.000%\n",
      "Epoch : 221 [0/5376 (0%)]\tLoss: 0.000016\t Accuracy:100.000%\n",
      "Epoch : 222 [0/5376 (0%)]\tLoss: 0.000015\t Accuracy:100.000%\n",
      "Epoch : 223 [0/5376 (0%)]\tLoss: 0.000015\t Accuracy:100.000%\n",
      "Epoch : 224 [0/5376 (0%)]\tLoss: 0.000015\t Accuracy:100.000%\n",
      "Epoch : 225 [0/5376 (0%)]\tLoss: 0.000015\t Accuracy:100.000%\n",
      "Epoch : 226 [0/5376 (0%)]\tLoss: 0.000014\t Accuracy:100.000%\n",
      "Epoch : 227 [0/5376 (0%)]\tLoss: 0.000014\t Accuracy:100.000%\n",
      "Epoch : 228 [0/5376 (0%)]\tLoss: 0.000014\t Accuracy:100.000%\n",
      "Epoch : 229 [0/5376 (0%)]\tLoss: 0.000014\t Accuracy:100.000%\n",
      "Epoch : 230 [0/5376 (0%)]\tLoss: 0.000013\t Accuracy:100.000%\n",
      "Epoch : 231 [0/5376 (0%)]\tLoss: 0.000013\t Accuracy:100.000%\n",
      "Epoch : 232 [0/5376 (0%)]\tLoss: 0.000013\t Accuracy:100.000%\n",
      "Epoch : 233 [0/5376 (0%)]\tLoss: 0.000013\t Accuracy:100.000%\n",
      "Epoch : 234 [0/5376 (0%)]\tLoss: 0.000012\t Accuracy:100.000%\n",
      "Epoch : 235 [0/5376 (0%)]\tLoss: 0.000012\t Accuracy:100.000%\n",
      "Epoch : 236 [0/5376 (0%)]\tLoss: 0.000012\t Accuracy:100.000%\n",
      "Epoch : 237 [0/5376 (0%)]\tLoss: 0.000012\t Accuracy:100.000%\n",
      "Epoch : 238 [0/5376 (0%)]\tLoss: 0.000012\t Accuracy:100.000%\n",
      "Epoch : 239 [0/5376 (0%)]\tLoss: 0.000012\t Accuracy:100.000%\n",
      "Epoch : 240 [0/5376 (0%)]\tLoss: 0.000011\t Accuracy:100.000%\n",
      "Epoch : 241 [0/5376 (0%)]\tLoss: 0.000011\t Accuracy:100.000%\n",
      "Epoch : 242 [0/5376 (0%)]\tLoss: 0.000011\t Accuracy:100.000%\n",
      "Epoch : 243 [0/5376 (0%)]\tLoss: 0.000011\t Accuracy:100.000%\n",
      "Epoch : 244 [0/5376 (0%)]\tLoss: 0.000011\t Accuracy:100.000%\n",
      "Epoch : 245 [0/5376 (0%)]\tLoss: 0.000010\t Accuracy:100.000%\n",
      "Epoch : 246 [0/5376 (0%)]\tLoss: 0.000010\t Accuracy:100.000%\n",
      "Epoch : 247 [0/5376 (0%)]\tLoss: 0.000010\t Accuracy:100.000%\n",
      "Epoch : 248 [0/5376 (0%)]\tLoss: 0.000010\t Accuracy:100.000%\n",
      "Epoch : 249 [0/5376 (0%)]\tLoss: 0.000010\t Accuracy:100.000%\n"
     ]
    }
   ],
   "source": [
    "loss_vec = fit(mlp, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_vec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "777eb068065662bea355507597b158eb8713b7f2"
   },
   "source": [
    "## MLP Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "a3aca2215610465fcccc58f1b98fc1d56096f364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:59.000% \n"
     ]
    }
   ],
   "source": [
    "def evaluate(model):\n",
    "#model = mlp\n",
    "    predict_list = []\n",
    "    correct = 0 \n",
    "    for test_imgs, test_labels in test_loader:\n",
    "        #print(test_imgs.shape)\n",
    "        test_imgs = Variable(test_imgs).float()\n",
    "        output = model(test_imgs)\n",
    "        predicted = torch.max(output,1)[1]\n",
    "        predict_list.append(predicted)\n",
    "        correct += (predicted == test_labels).sum()\n",
    "    print(\"Test accuracy:{:.3f}% \".format( float(correct)*100 / (len(test_loader)*BATCH_SIZE)))\n",
    "predict = evaluate(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "810c7b5869b3c704d5a5a7e28289133c27c5790c"
   },
   "source": [
    "<center><h2>Convolutional Neural Network</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0dd01423ecc2b3c46c42aaa033ad8556ba029dc9"
   },
   "source": [
    "## Explanation\n",
    "\n",
    "To better understand convolutional neural network I recommend the great section on it here : http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "**Convolutional operation** : First let's clarify briefly how we can perform the convolutional operation on an image. For that we need to define a **kernel** which is a small matrix of size 5 \\* 5 for example. To perform the convolution operation, we just need to slide the kernel along the image horizontally and vertically and do the dot product of the kernel and the small portion of the image.\n",
    "\n",
    "**Pooling** : the convolutional operation give an output of the same size of the input image. To reduce the size of the image and thus reduce the number of paramers in the model we perform a Pooling operation. The pooling operation need a window size.. By sliding the window along the image, we compute the mean or the max of the portion of the image inside the window in case of MeanPooling or MaxPooling.\n",
    "\n",
    "**Stride** is the number of pixels to pass at a time when sliding the convolutional kernel.  \n",
    "\n",
    "**Padding** to preserve exactly the size of the input image, it is useful to add a zero padding on the border of the image. \n",
    "\n",
    "\n",
    "**To remember** : What makes a CNN so interesting for images is that it is invariant by translation and for each convolutional layer we only need to store the kernels. Thus we can stack a lot of layers to learn deep features without having too much parameters that would make a model untrainnable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9baaa52b6b9b6b8f8287a10fcc801606848fb726"
   },
   "source": [
    "## Data loader\n",
    "\n",
    "Since a CNN needs a image shape as input let's reshape our flatten images to real image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "5e84347012cd6abc2f6f46299aab08b0c89563ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5376, 1, 50, 50])\n",
      "torch.Size([1344, 1, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "torch_X_train = torch_X_train.view(-1, 1,n_width,n_height).float()\n",
    "torch_X_test = torch_X_test.view(-1,1,n_width,n_height).float()\n",
    "print(torch_X_train.shape)\n",
    "print(torch_X_test.shape)\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
    "test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_uuid": "c669e5f557724154d9f4f5f8b5d0d6c9b676d551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=5184, out_features=125, bias=True)\n",
      "  (fc2): Linear(in_features=125, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([200, 1, 50, 50])\n",
      "torch.Size([200, 10])\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(32,64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(9*9*64,125)\n",
    "        self.fc2 =nn.Linear(125,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x),2))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.view(-1,9*9*64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    " \n",
    "cnn = CNN()\n",
    "print(cnn)\n",
    "\n",
    "it = iter(train_loader)\n",
    "X_batch, y_batch = next(it)\n",
    "print(X_batch.shape)\n",
    "print(cnn.forward(X_batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_uuid": "797cbe3cff05d8bdb42f3273a06839b18daf5266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/5376 (0%)]\tLoss: 2.300618\t Accuracy:11.000%\n",
      "Epoch : 1 [0/5376 (0%)]\tLoss: 2.141267\t Accuracy:24.000%\n",
      "Epoch : 2 [0/5376 (0%)]\tLoss: 1.844463\t Accuracy:37.000%\n",
      "Epoch : 3 [0/5376 (0%)]\tLoss: 1.667828\t Accuracy:45.500%\n",
      "Epoch : 4 [0/5376 (0%)]\tLoss: 1.402853\t Accuracy:53.000%\n",
      "Epoch : 5 [0/5376 (0%)]\tLoss: 1.285275\t Accuracy:56.500%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Blade\\OneDrive - University of Florida\\Coding\\CNN_Learning_2\\cnn-with-pytorch-for-mnist.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Blade/OneDrive%20-%20University%20of%20Florida/Coding/CNN_Learning_2/cnn-with-pytorch-for-mnist.ipynb#ch0000026?line=0'>1</a>\u001b[0m fit(cnn,train_loader)\n",
      "\u001b[1;32mc:\\Users\\Blade\\OneDrive - University of Florida\\Coding\\CNN_Learning_2\\cnn-with-pytorch-for-mnist.ipynb Cell 15'\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, train_loader)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Blade/OneDrive%20-%20University%20of%20Florida/Coding/CNN_Learning_2/cnn-with-pytorch-for-mnist.ipynb#ch0000016?line=12'>13</a>\u001b[0m output \u001b[39m=\u001b[39m model(var_X_batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Blade/OneDrive%20-%20University%20of%20Florida/Coding/CNN_Learning_2/cnn-with-pytorch-for-mnist.ipynb#ch0000016?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m error(output, var_y_batch)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Blade/OneDrive%20-%20University%20of%20Florida/Coding/CNN_Learning_2/cnn-with-pytorch-for-mnist.ipynb#ch0000016?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Blade/OneDrive%20-%20University%20of%20Florida/Coding/CNN_Learning_2/cnn-with-pytorch-for-mnist.ipynb#ch0000016?line=15'>16</a>\u001b[0m lossvec\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Blade/OneDrive%20-%20University%20of%20Florida/Coding/CNN_Learning_2/cnn-with-pytorch-for-mnist.ipynb#ch0000016?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\hw\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\hw\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Blade/Anaconda3/envs/hw/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit(cnn,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "890dfc8d29ac70df919807b275b38112ca9297ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:10.071% \n"
     ]
    }
   ],
   "source": [
    "evaluate(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2556, -2.3080, -2.3297,  ..., -2.3225, -2.3270, -2.3085],\n",
      "        [-2.2557, -2.3084, -2.3296,  ..., -2.3227, -2.3274, -2.3082],\n",
      "        [-2.2759, -2.3518, -2.3248,  ..., -2.3471, -2.3643, -2.2778],\n",
      "        ...,\n",
      "        [-2.2575, -2.3123, -2.3292,  ..., -2.3249, -2.3307, -2.3055],\n",
      "        [-2.2759, -2.3518, -2.3248,  ..., -2.3471, -2.3643, -2.2778],\n",
      "        [-2.2759, -2.3518, -2.3248,  ..., -2.3471, -2.3643, -2.2778]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([200, 1, 300, 300])\n",
      "torch.Size([200, 10])\n",
      "torch.Size([200])\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "output = cnn.forward(X_batch)\n",
    "predicted = torch.max(output.data,1)[1]\n",
    "print(output)\n",
    "print(X_batch.shape)\n",
    "print(output.shape)\n",
    "print(predicted.shape)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "predicted = torch.max(output.data,1)[1]\n",
    "print(predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
